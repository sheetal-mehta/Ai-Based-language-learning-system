{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Importing libraries"
      ],
      "metadata": {
        "id": "YUZEl3rmk50L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snrqo7JFoTG9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import audio_dataset_from_directory\n",
        "import numpy as np\n",
        "import librosa\n",
        "\n",
        "#tf.compat.v1.disable_eager_execution()\n",
        "#import tensorflow_io as tfio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Downloading and splitting data"
      ],
      "metadata": {
        "id": "9LxiXyE3lDC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the input data from drive (eins, zwei, drei folders)\n",
        "!gdown 10P678fWDyAJIRv_HlqsXtS2u68NTFZ7I\n",
        "!unzip data_cnn.zip"
      ],
      "metadata": {
        "id": "IVP_uoLzHxvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_dir = \"/content/data_cnn\"\n",
        "\n",
        "train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
        "    directory=data_dir,\n",
        "    batch_size=None,\n",
        "    validation_split=0.2,\n",
        "    seed=0,\n",
        "    subset='both')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fspYPv7xHxl8",
        "outputId": "f305cb43-8c50-428e-fadc-f27a4dc5c8b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 526 files belonging to 3 classes.\n",
            "Using 421 files for training.\n",
            "Using 105 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing data"
      ],
      "metadata": {
        "id": "I1kZWd-rlIfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_audio_mfps(audio, label):\n",
        "    # Convert audio tensor to a compatible format\n",
        "    audio = tf.cast(audio, tf.float32)  # Cast audio to float32\n",
        "    audio = audio / 32768.0  # Normalize audio\n",
        "\n",
        "    # Extract mel-frequency power spectra\n",
        "    def _extract_mel(audio):\n",
        "        # Reshape the audio tensor to (batch_size, num_samples) as expected by tf.signal.stft\n",
        "        audio = tf.reshape(audio, [-1])\n",
        "\n",
        "        # Compute mel-frequency power spectra\n",
        "        stfts = tf.signal.stft(audio, frame_length=1024, frame_step=512, fft_length=1024)\n",
        "        spectrograms = tf.abs(stfts)\n",
        "\n",
        "        num_spectrogram_bins = stfts.shape[-1]\n",
        "        lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 128\n",
        "        linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
        "            num_mel_bins, num_spectrogram_bins, 16000, lower_edge_hertz, upper_edge_hertz)\n",
        "\n",
        "        mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\n",
        "\n",
        "        return mel_spectrograms\n",
        "\n",
        "    # Use tf.py_function to call _extract_mel with audio tensor\n",
        "    mel_spectra = tf.py_function(_extract_mel, [audio], tf.float32)\n",
        "\n",
        "    return mel_spectra, label"
      ],
      "metadata": {
        "id": "zwn8sqYnHxic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_processed_mspec = train_ds.map(preprocess_audio_mfps)\n",
        "val_processed_mspec = val_ds.map(preprocess_audio_mfps)"
      ],
      "metadata": {
        "id": "gEzAbZHVHxfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Find the maximum sequence length in the training dataset\n",
        "max_length = max(len(seq) for seq, _ in train_processed_mspec.as_numpy_iterator())\n",
        "\n",
        "# Function to pad sequences\n",
        "def pad_sequence(seq, label):\n",
        "    padded_seq = tf.pad(seq, paddings=[[0, max_length - tf.shape(seq)[0]], [0, 0]])\n",
        "    return padded_seq, label\n",
        "\n",
        "# Pad the training dataset\n",
        "padded_train_ds_mfps = train_processed_mspec.map(pad_sequence)\n",
        "\n",
        "# Pad the validation dataset\n",
        "padded_val_ds_mfps = val_processed_mspec.map(pad_sequence)"
      ],
      "metadata": {
        "id": "BRTe_R8aIOq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_pairs_and_labels(padded_dataset):\n",
        "    pairs = []\n",
        "    labels = []\n",
        "\n",
        "    for sequence, label in padded_dataset:\n",
        "        # Assuming 'sequence' is your padded sequence and 'label' is its corresponding label\n",
        "        pairs.append(sequence)\n",
        "        labels.append(label)\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    pairs = np.array(pairs)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    return pairs, labels\n",
        "\n",
        "# Apply the function to your padded datasets\n",
        "train_data, train_labels = create_pairs_and_labels(padded_train_ds_mfps)\n",
        "val_data, val_labels = create_pairs_and_labels(padded_val_ds_mfps)\n"
      ],
      "metadata": {
        "id": "pgDf3a9CYfEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating pairs input for siamese network"
      ],
      "metadata": {
        "id": "FKCnHahalPJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have your dataset X and corresponding labels y\n",
        "# X.shape = (num_samples, input_vector_size)\n",
        "# y.shape = (num_samples,)\n",
        "\n",
        "# Function to create pairs of data and labels\n",
        "def create_pairs(X, y, num_pairs):\n",
        "    pairs = []\n",
        "    labels = []\n",
        "    num_classes = len(np.unique(y))\n",
        "    class_indices = [np.where(y == i)[0] for i in range(num_classes)]\n",
        "\n",
        "    for _ in range(num_pairs):\n",
        "        # Select a random class (label)\n",
        "        class_idx = np.random.randint(0, num_classes)\n",
        "        # Select a random sample from the selected class\n",
        "        idx_1 = np.random.choice(class_indices[class_idx])\n",
        "        # Ensure that the second sample is from the same class for half of the pairs\n",
        "        should_be_same_class = np.random.randint(0, 2)\n",
        "        if should_be_same_class:\n",
        "            idx_2 = np.random.choice(class_indices[class_idx])\n",
        "        else:\n",
        "            # Select a random class different from the first one\n",
        "            class_idx_2 = (class_idx + np.random.randint(1, num_classes)) % num_classes\n",
        "            idx_2 = np.random.choice(class_indices[class_idx_2])\n",
        "        pairs.append([X[idx_1], X[idx_2]])\n",
        "        # 1 if same class, 0 if different class\n",
        "        labels.append(1 if should_be_same_class else 0)\n",
        "\n",
        "    return np.array(pairs), np.array(labels)\n",
        "\n",
        "# Example usage:\n",
        "num_pairs = 1500  # Adjust this number based on your dataset size and requirements\n",
        "train_pairs, train_pairs_labels = create_pairs(train_data, train_labels, num_pairs)\n",
        "num_pairs_val = 500\n",
        "val_pairs, val_pairs_labels = create_pairs(val_data, val_labels, num_pairs_val)"
      ],
      "metadata": {
        "id": "ei60gQ1bZvE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rnvAmgoToZEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_pairs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQ9Q5lTybv-s",
        "outputId": "9985cc13-94d5-4aab-8d48-e7108c516e84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1500, 2, 402, 128)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_pairs_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7TABvH-b384",
        "outputId": "6216d14d-f8fa-4754-cc02-99b5bbb3ddf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating model architecture"
      ],
      "metadata": {
        "id": "NsRXbngklWAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, MaxPooling1D, GlobalAveragePooling1D, Dense, Lambda\n",
        "from tensorflow.keras import backend as K\n",
        "# Define Siamese network architecture\n",
        "def siamese_model(input_shape):\n",
        "    input = Input(shape=input_shape)\n",
        "    x = Conv1D(16, kernel_size=3, activation='relu', padding='same')(input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv1D(16, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling1D(pool_size=2, strides=2)(x)\n",
        "\n",
        "    x = Conv1D(32, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv1D(32, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling1D(pool_size=2, strides=2)(x)\n",
        "\n",
        "    x = Conv1D(64, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv1D(64, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling1D(pool_size=2, strides=2)(x)\n",
        "\n",
        "    x = Conv1D(128, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv1D(128, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling1D(pool_size=2, strides=2)(x)\n",
        "\n",
        "    x = Conv1D(256, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv1D(256, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling1D(pool_size=2, strides=2)(x)\n",
        "\n",
        "    x = Conv1D(512, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = Conv1D(1024, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    return Model(input, x)\n",
        "\n",
        "# Define cosine similarity function\n",
        "def cosine_similarity(vectors):\n",
        "    x, y = vectors\n",
        "    x = K.l2_normalize(x, axis=-1)\n",
        "    y = K.l2_normalize(y, axis=-1)\n",
        "    return K.sum(x * y, axis=-1, keepdims=True)\n",
        "\n",
        "# Define contrastive loss function\n",
        "def contrastive_loss(y_true, y_pred):\n",
        "    margin = 1.0\n",
        "    return K.mean(y_true * K.square(1 - y_pred) + (1 - y_true) * K.square(K.maximum(y_pred - margin, 0)))\n",
        "\n",
        "# Create Siamese model\n",
        "input_shape = (402,128) # Define the shape of your input vectors\n",
        "base_model = siamese_model(input_shape)\n",
        "\n",
        "input_a = Input(shape=input_shape)\n",
        "input_b = Input(shape=input_shape)\n",
        "\n",
        "processed_a = base_model(input_a)\n",
        "processed_b = base_model(input_b)\n",
        "\n",
        "cosine_sim = Lambda(cosine_similarity, output_shape=(1,))([processed_a, processed_b])\n",
        "\n",
        "siamese_network = Model(inputs=[input_a, input_b], outputs=cosine_sim)\n",
        "\n",
        "# Compile the Siamese model with contrastive loss\n",
        "siamese_network.compile(optimizer='adam', loss=contrastive_loss, metrics=['accuracy'])\n",
        "\n",
        "# Summary of the Siamese model\n",
        "siamese_network.summary()\n"
      ],
      "metadata": {
        "id": "aw_n0unToXbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2bf66dd-dbe5-4e69-8b8e-422a7df0863d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 402, 128)]           0         []                            \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)        [(None, 402, 128)]           0         []                            \n",
            "                                                                                                  \n",
            " model (Functional)          (None, 1024)                 2377312   ['input_2[0][0]',             \n",
            "                                                                     'input_3[0][0]']             \n",
            "                                                                                                  \n",
            " lambda (Lambda)             (None, 1)                    0         ['model[0][0]',               \n",
            "                                                                     'model[1][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2377312 (9.07 MB)\n",
            "Trainable params: 2372256 (9.05 MB)\n",
            "Non-trainable params: 5056 (19.75 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert labels to integers\n",
        "train_pairs_labels = train_pairs_labels.astype(np.float32)\n",
        "val_pairs_labels = val_pairs_labels.astype(np.float32)\n"
      ],
      "metadata": {
        "id": "SjSU-WYIi1L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "batch_size=32\n",
        "\n",
        "# Define callbacks\n",
        "checkpoint_callback = ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "\n",
        "history = siamese_network.fit(\n",
        "    [train_pairs[:, 0], train_pairs[:, 1]],  # input pairs\n",
        "    train_pairs_labels,\n",
        "    epochs=50,\n",
        "    batch_size=batch_size,\n",
        "    validation_data=([val_pairs[:, 0], val_pairs[:, 1]], val_pairs_labels),\n",
        "    callbacks=[checkpoint_callback, early_stopping_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQNe2msEGC7x",
        "outputId": "0631e6cb-b9bb-4dc7-e2fb-f0633aa28546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 1500 samples, validate on 500 samples\n",
            "Epoch 1/50\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 0.2719 - accuracy: 0.6827"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_accuracy improved from -inf to 0.51200, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1500/1500 [==============================] - 36s 24ms/sample - loss: 0.2719 - accuracy: 0.6827 - val_loss: 2.5011e-15 - val_accuracy: 0.5120\n",
            "Epoch 2/50\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 0.0159 - accuracy: 0.8313\n",
            "Epoch 2: val_accuracy did not improve from 0.51200\n",
            "1500/1500 [==============================] - 33s 22ms/sample - loss: 0.0159 - accuracy: 0.8313 - val_loss: 3.1903e-15 - val_accuracy: 0.5120\n",
            "Epoch 3/50\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.8427\n",
            "Epoch 3: val_accuracy did not improve from 0.51200\n",
            "1500/1500 [==============================] - 32s 21ms/sample - loss: 0.0015 - accuracy: 0.8427 - val_loss: 4.0952e-12 - val_accuracy: 0.5120\n",
            "Epoch 4/50\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 2.9582e-04 - accuracy: 0.8440\n",
            "Epoch 4: val_accuracy did not improve from 0.51200\n",
            "1500/1500 [==============================] - 31s 21ms/sample - loss: 2.9582e-04 - accuracy: 0.8440 - val_loss: 2.0446e-08 - val_accuracy: 0.5120\n",
            "Epoch 5/50\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 0.8407\n",
            "Epoch 5: val_accuracy improved from 0.51200 to 0.51400, saving model to best_model.h5\n",
            "1500/1500 [==============================] - 31s 21ms/sample - loss: 0.0029 - accuracy: 0.8407 - val_loss: 0.0044 - val_accuracy: 0.5140\n",
            "Epoch 6/50\n",
            "1500/1500 [==============================] - ETA: 0s - loss: 0.0066 - accuracy: 0.8407\n",
            "Epoch 6: val_accuracy did not improve from 0.51400\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "1500/1500 [==============================] - 34s 22ms/sample - loss: 0.0066 - accuracy: 0.8407 - val_loss: 0.0065 - val_accuracy: 0.5060\n",
            "Epoch 6: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xWVOWOVVimvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##More number of pairs"
      ],
      "metadata": {
        "id": "AIhRdDWyoa_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Assuming you have your dataset X and corresponding labels y\n",
        "# X.shape = (num_samples, input_vector_size)\n",
        "# y.shape = (num_samples,)\n",
        "\n",
        "# Function to create pairs of data and labels\n",
        "def create_pairs(X, y, num_pairs):\n",
        "    pairs = []\n",
        "    labels = []\n",
        "    num_classes = len(np.unique(y))\n",
        "    class_indices = [np.where(y == i)[0] for i in range(num_classes)]\n",
        "\n",
        "    for _ in range(num_pairs):\n",
        "        # Select a random class (label)\n",
        "        class_idx = np.random.randint(0, num_classes)\n",
        "        # Select a random sample from the selected class\n",
        "        idx_1 = np.random.choice(class_indices[class_idx])\n",
        "        # Ensure that the second sample is from the same class for half of the pairs\n",
        "        should_be_same_class = np.random.randint(0, 2)\n",
        "        if should_be_same_class:\n",
        "            idx_2 = np.random.choice(class_indices[class_idx])\n",
        "        else:\n",
        "            # Select a random class different from the first one\n",
        "            class_idx_2 = (class_idx + np.random.randint(1, num_classes)) % num_classes\n",
        "            idx_2 = np.random.choice(class_indices[class_idx_2])\n",
        "        pairs.append([X[idx_1], X[idx_2]])\n",
        "        # 1 if same class, 0 if different class\n",
        "        labels.append(1 if should_be_same_class else 0)\n",
        "\n",
        "    return np.array(pairs), np.array(labels)\n",
        "\n",
        "# Example usage:\n",
        "num_pairs = 4000  # Adjust this number based on your dataset size and requirements\n",
        "train_pairs, train_pairs_labels = create_pairs(train_data, train_labels, num_pairs)\n",
        "num_pairs_val = 1000\n",
        "val_pairs, val_pairs_labels = create_pairs(val_data, val_labels, num_pairs_val)\n",
        "\n",
        "\n",
        "# Convert labels to integers\n",
        "train_pairs_labels = train_pairs_labels.astype(np.float32)\n",
        "val_pairs_labels = val_pairs_labels.astype(np.float32)\n"
      ],
      "metadata": {
        "id": "QKWmRAySodbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "batch_size=32\n",
        "\n",
        "# Define callbacks\n",
        "checkpoint_callback = ModelCheckpoint(filepath='best_model_5k.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "\n",
        "history = siamese_network.fit(\n",
        "    [train_pairs[:, 0], train_pairs[:, 1]],  # input pairs\n",
        "    train_pairs_labels,\n",
        "    epochs=50,\n",
        "    batch_size=batch_size,\n",
        "    validation_data=([val_pairs[:, 0], val_pairs[:, 1]], val_pairs_labels),\n",
        "    callbacks=[checkpoint_callback, early_stopping_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrluuShaofec",
        "outputId": "246a1a67-2ee0-4b63-a53f-81167fb108ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0962 - accuracy: 0.7862\n",
            "Epoch 1: val_accuracy improved from -inf to 0.48500, saving model to best_model_5k.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r125/125 [==============================] - 96s 703ms/step - loss: 0.0962 - accuracy: 0.7862 - val_loss: 1.1755e-08 - val_accuracy: 0.4850\n",
            "Epoch 2/50\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.8372\n",
            "Epoch 2: val_accuracy did not improve from 0.48500\n",
            "125/125 [==============================] - 87s 691ms/step - loss: 0.0079 - accuracy: 0.8372 - val_loss: 0.0163 - val_accuracy: 0.4830\n",
            "Epoch 3/50\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0059 - accuracy: 0.8410\n",
            "Epoch 3: val_accuracy did not improve from 0.48500\n",
            "125/125 [==============================] - 85s 679ms/step - loss: 0.0059 - accuracy: 0.8410 - val_loss: 0.0404 - val_accuracy: 0.4800\n",
            "Epoch 4/50\n",
            "125/125 [==============================] - ETA: 0s - loss: 0.0058 - accuracy: 0.8407\n",
            "Epoch 4: val_accuracy did not improve from 0.48500\n",
            "125/125 [==============================] - 85s 680ms/step - loss: 0.0058 - accuracy: 0.8407 - val_loss: 0.0033 - val_accuracy: 0.4810\n",
            "Epoch 5/50\n",
            "125/125 [==============================] - ETA: 0s - loss: 9.8881e-04 - accuracy: 0.8440\n",
            "Epoch 5: val_accuracy improved from 0.48500 to 0.77600, saving model to best_model_5k.h5\n",
            "125/125 [==============================] - 85s 682ms/step - loss: 9.8881e-04 - accuracy: 0.8440 - val_loss: 0.0716 - val_accuracy: 0.7760\n",
            "Epoch 6/50\n",
            "125/125 [==============================] - ETA: 0s - loss: 1.5918e-04 - accuracy: 0.8445\n",
            "Epoch 6: val_accuracy improved from 0.77600 to 0.81300, saving model to best_model_5k.h5\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "125/125 [==============================] - 95s 761ms/step - loss: 1.5918e-04 - accuracy: 0.8445 - val_loss: 0.0263 - val_accuracy: 0.8130\n",
            "Epoch 6: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "batch_size=32\n",
        "\n",
        "# Define callbacks\n",
        "checkpoint_callback = ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "\n",
        "history = siamese_network.fit(\n",
        "    [train_pairs[:, 0], train_pairs[:, 1]],  # input pairs\n",
        "    train_pairs_labels,\n",
        "    epochs=50,\n",
        "    batch_size=batch_size,\n",
        "    validation_data=([val_pairs[:, 0], val_pairs[:, 1]], val_pairs_labels),\n",
        "    callbacks=[checkpoint_callback, early_stopping_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGroa8fglyws",
        "outputId": "2646217f-8fa9-46eb-fc75-0e946306ad96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.8128\n",
            "Epoch 1: val_accuracy improved from -inf to 0.52700, saving model to best_model.h5\n",
            "79/79 [==============================] - 58s 735ms/step - loss: 0.0275 - accuracy: 0.8128 - val_loss: 1.5236e-05 - val_accuracy: 0.5270\n",
            "Epoch 2/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79/79 [==============================] - ETA: 0s - loss: 0.0051 - accuracy: 0.8272\n",
            "Epoch 2: val_accuracy did not improve from 0.52700\n",
            "79/79 [==============================] - 55s 700ms/step - loss: 0.0051 - accuracy: 0.8272 - val_loss: 9.1101e-07 - val_accuracy: 0.5270\n",
            "Epoch 3/50\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 0.8308\n",
            "Epoch 3: val_accuracy improved from 0.52700 to 0.53300, saving model to best_model.h5\n",
            "79/79 [==============================] - 57s 722ms/step - loss: 0.0015 - accuracy: 0.8308 - val_loss: 0.0036 - val_accuracy: 0.5330\n",
            "Epoch 4/50\n",
            "79/79 [==============================] - ETA: 0s - loss: 2.9538e-04 - accuracy: 0.8316\n",
            "Epoch 4: val_accuracy improved from 0.53300 to 0.76700, saving model to best_model.h5\n",
            "79/79 [==============================] - 85s 1s/step - loss: 2.9538e-04 - accuracy: 0.8316 - val_loss: 0.0806 - val_accuracy: 0.7670\n",
            "Epoch 5/50\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 0.8268\n",
            "Epoch 5: val_accuracy did not improve from 0.76700\n",
            "79/79 [==============================] - 58s 737ms/step - loss: 0.0029 - accuracy: 0.8268 - val_loss: 0.0618 - val_accuracy: 0.7200\n",
            "Epoch 6/50\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 0.8304\n",
            "Epoch 6: val_accuracy did not improve from 0.76700\n",
            "79/79 [==============================] - 56s 708ms/step - loss: 0.0018 - accuracy: 0.8304 - val_loss: 0.1015 - val_accuracy: 0.6390\n",
            "Epoch 7/50\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.0146 - accuracy: 0.8208\n",
            "Epoch 7: val_accuracy did not improve from 0.76700\n",
            "Restoring model weights from the end of the best epoch: 2.\n",
            "79/79 [==============================] - 56s 705ms/step - loss: 0.0146 - accuracy: 0.8208 - val_loss: 0.0165 - val_accuracy: 0.5330\n",
            "Epoch 7: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run on 02-06-24\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "batch_size=32\n",
        "\n",
        "# Define callbacks\n",
        "checkpoint_callback = ModelCheckpoint(filepath='best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "\n",
        "history = siamese_network.fit(\n",
        "    [train_pairs[:, 0], train_pairs[:, 1]],  # input pairs\n",
        "    train_pairs_labels,\n",
        "    epochs=50,\n",
        "    batch_size=batch_size,\n",
        "    validation_data=([val_pairs[:, 0], val_pairs[:, 1]], val_pairs_labels),\n",
        "    callbacks=[checkpoint_callback, early_stopping_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GTsdLRZioJT",
        "outputId": "5c9727ca-e89d-47d8-cc8f-3bfa4bf38fd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1837 - accuracy: 0.7348\n",
            "Epoch 1: val_accuracy improved from -inf to 0.49900, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r79/79 [==============================] - 68s 748ms/step - loss: 0.1837 - accuracy: 0.7348 - val_loss: 2.7320e-15 - val_accuracy: 0.4990\n",
            "Epoch 2/50\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 0.8404\n",
            "Epoch 2: val_accuracy did not improve from 0.49900\n",
            "79/79 [==============================] - 57s 716ms/step - loss: 0.0072 - accuracy: 0.8404 - val_loss: 1.3089e-09 - val_accuracy: 0.4990\n",
            "Epoch 3/50\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.0163 - accuracy: 0.8344\n",
            "Epoch 3: val_accuracy did not improve from 0.49900\n",
            "79/79 [==============================] - 55s 693ms/step - loss: 0.0163 - accuracy: 0.8344 - val_loss: 3.5578e-07 - val_accuracy: 0.4990\n",
            "Epoch 4/50\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.8372\n",
            "Epoch 4: val_accuracy did not improve from 0.49900\n",
            "79/79 [==============================] - 55s 699ms/step - loss: 0.0117 - accuracy: 0.8372 - val_loss: 1.0030e-04 - val_accuracy: 0.4990\n",
            "Epoch 5/50\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.0031 - accuracy: 0.8440\n",
            "Epoch 5: val_accuracy improved from 0.49900 to 0.67400, saving model to best_model.h5\n",
            "79/79 [==============================] - 56s 711ms/step - loss: 0.0031 - accuracy: 0.8440 - val_loss: 0.1365 - val_accuracy: 0.6740\n",
            "Epoch 6/50\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.0012 - accuracy: 0.8436\n",
            "Epoch 6: val_accuracy improved from 0.67400 to 0.70400, saving model to best_model.h5\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "79/79 [==============================] - 55s 702ms/step - loss: 0.0012 - accuracy: 0.8436 - val_loss: 0.1259 - val_accuracy: 0.7040\n",
            "Epoch 6: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import soundfile as sf\n",
        "import audioread\n",
        "import numpy as np\n",
        "\n",
        "# Function to load M4A file\n",
        "def load_m4a(filename):\n",
        "    with audioread.audio_open(filename) as f:\n",
        "        data = np.hstack([np.frombuffer(chunk, dtype='int16') for chunk in f])\n",
        "        return data, f.samplerate\n",
        "\n",
        "# Load your M4A file\n",
        "data, samplerate = load_m4a(\"/content/Anse.m4a\")\n",
        "\n",
        "# Convert it to WAV using soundfile\n",
        "sf.write(\"Anse_wav.wav\", data, samplerate, format='WAV', subtype='PCM_16')\n"
      ],
      "metadata": {
        "id": "xzvREVNUooe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_audio_file(file_path):\n",
        "    # Load an audio file as a tensor, assume the file is a WAV file\n",
        "    audio_binary = tf.io.read_file(file_path)\n",
        "    audio, sample_rate = tf.audio.decode_wav(audio_binary)\n",
        "    # Only use the first channel if it's stereo\n",
        "    audio = audio[:, 0]\n",
        "    return audio, sample_rate\n",
        "\n",
        "\n",
        "def preprocess_audio_mfps(audio, sample_rate):\n",
        "    # Cast audio to float32 and normalize\n",
        "    audio = tf.cast(audio, tf.float32)\n",
        "    audio = audio / 32768.0  # Normalize audio\n",
        "\n",
        "    # Extract mel-frequency power spectra\n",
        "    def _extract_mel(audio):\n",
        "        # Compute mel-frequency power spectra\n",
        "        stfts = tf.signal.stft(audio, frame_length=1024, frame_step=512, fft_length=1024)\n",
        "        spectrograms = tf.abs(stfts)\n",
        "\n",
        "        num_spectrogram_bins = stfts.shape[-1]\n",
        "        lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 128\n",
        "        linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
        "            num_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz, upper_edge_hertz)\n",
        "\n",
        "        mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\n",
        "        mel_spectrograms = tf.reshape(mel_spectrograms, [1, -1, 128])  # Reshape for batch dimension if needed\n",
        "        return mel_spectrograms\n",
        "\n",
        "    # Use tf.py_function to allow for eager execution of the extraction\n",
        "    mel_spectra = tf.py_function(_extract_mel, [audio], tf.float32)\n",
        "    return mel_spectra\n",
        "\n",
        "def pad_sequence(seq):\n",
        "    # Pad the sequence to the maximum length found in the training data\n",
        "    padded_seq = tf.pad(seq, paddings=[[0, 0], [0, max_length - tf.shape(seq)[1]], [0, 0]], constant_values=0)\n",
        "    return padded_seq\n"
      ],
      "metadata": {
        "id": "I2qVmAJ9kVGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def contrastive_loss(y_true, y_pred, margin=1):\n",
        "    # Calculate the Euclidean distance between the two outputs\n",
        "    square_pred = tf.square(y_pred)\n",
        "    margin_square = tf.square(tf.maximum(margin - y_pred, 0))\n",
        "    return tf.reduce_mean(y_true * square_pred + (1 - y_true) * margin_square)"
      ],
      "metadata": {
        "id": "XFghB0vVwS2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of how to use these functions\n",
        "file_path = '/content/zwei_wav.wav'\n",
        "audio, sample_rate = load_audio_file(file_path)\n",
        "processed_audio = preprocess_audio_mfps(audio, sample_rate)\n",
        "processed_padded = pad_sequence(processed_audio)\n",
        "\n",
        "file_path_base = '/content/Q1200901.wav'\n",
        "audio_base, sample_rate_base = load_audio_file(file_path_base)\n",
        "processed_audio_base = preprocess_audio_mfps(audio_base, sample_rate_base)\n",
        "processed_padded_base = pad_sequence(processed_audio_base)\n",
        "\n",
        "\n",
        "# # Load the saved Siamese model\n",
        "# model_path = 'best_model.h5'\n",
        "# siamese_model = tf.keras.models.load_model(model_path)\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Register the custom loss function and load the model\n",
        "siamese_model = load_model('best_model.h5', custom_objects={'contrastive_loss': contrastive_loss})\n",
        "\n",
        "# Suppose you have another processed audio tensor, reference_audio, to compare against\n",
        "# Here you should provide your model with both samples as a pair\n",
        "output = siamese_model([processed_padded, processed_padded_base])  # Assuming your model takes a list of two inputs\n",
        "\n",
        "# The output typically could be a similarity score or a classification result\n",
        "print(\"Model output:\", output.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iXCJ2HOsRtZ",
        "outputId": "8a4b6a9a-7549-424a-d4ed-3dae433641da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output: [[0.97178507]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of how to use these functions\n",
        "file_path = '/content/Anse_wav.wav'\n",
        "audio, sample_rate = load_audio_file(file_path)\n",
        "processed_audio = preprocess_audio_mfps(audio, sample_rate)\n",
        "processed_padded = pad_sequence(processed_audio)\n",
        "\n",
        "file_path_base = '/content/Q1200901.wav'\n",
        "audio_base, sample_rate_base = load_audio_file(file_path_base)\n",
        "processed_audio_base = preprocess_audio_mfps(audio_base, sample_rate_base)\n",
        "processed_padded_base = pad_sequence(processed_audio_base)\n",
        "\n",
        "\n",
        "# # Load the saved Siamese model\n",
        "# model_path = 'best_model.h5'\n",
        "# siamese_model = tf.keras.models.load_model(model_path)\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Register the custom loss function and load the model\n",
        "siamese_model = load_model('best_model.h5', custom_objects={'contrastive_loss': contrastive_loss})\n",
        "\n",
        "# Suppose you have another processed audio tensor, reference_audio, to compare against\n",
        "# Here you should provide your model with both samples as a pair\n",
        "output = siamese_model([processed_padded, processed_padded_base])  # Assuming your model takes a list of two inputs\n",
        "\n",
        "# The output typically could be a similarity score or a classification result\n",
        "print(\"Model output:\", output.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNfkuhHBmwUy",
        "outputId": "d2d427df-cc28-448d-df61-4340e0eae4fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output: [[0.47611064]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of how to use these functions\n",
        "file_path = '/content/Jaii_wav.wav'\n",
        "audio, sample_rate = load_audio_file(file_path)\n",
        "processed_audio = preprocess_audio_mfps(audio, sample_rate)\n",
        "processed_padded = pad_sequence(processed_audio)\n",
        "\n",
        "file_path_base = '/content/Q1200901.wav'\n",
        "audio_base, sample_rate_base = load_audio_file(file_path_base)\n",
        "processed_audio_base = preprocess_audio_mfps(audio_base, sample_rate_base)\n",
        "processed_padded_base = pad_sequence(processed_audio_base)\n",
        "\n",
        "\n",
        "# # Load the saved Siamese model\n",
        "# model_path = 'best_model.h5'\n",
        "# siamese_model = tf.keras.models.load_model(model_path)\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Register the custom loss function and load the model\n",
        "siamese_model = load_model('best_model.h5', custom_objects={'contrastive_loss': contrastive_loss})\n",
        "\n",
        "# Suppose you have another processed audio tensor, reference_audio, to compare against\n",
        "# Here you should provide your model with both samples as a pair\n",
        "output = siamese_model([processed_padded, processed_padded_base])  # Assuming your model takes a list of two inputs\n",
        "\n",
        "# The output typically could be a similarity score or a classification result\n",
        "print(\"Model output:\", output.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kS2xCXrGw23J",
        "outputId": "a757862a-de30-47ec-9eaf-a91fd1cef7be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output: [[0.98684275]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of how to use these functions\n",
        "file_path = '/content/Jaii_wav.wav'\n",
        "audio, sample_rate = load_audio_file(file_path)\n",
        "processed_audio = preprocess_audio_mfps(audio, sample_rate)\n",
        "processed_padded = pad_sequence(processed_audio)\n",
        "\n",
        "file_path_base = '/content/Q1200901.wav'\n",
        "audio_base, sample_rate_base = load_audio_file(file_path_base)\n",
        "processed_audio_base = preprocess_audio_mfps(audio_base, sample_rate_base)\n",
        "processed_padded_base = pad_sequence(processed_audio_base)\n",
        "\n",
        "\n",
        "# # Load the saved Siamese model\n",
        "# model_path = 'best_model.h5'\n",
        "# siamese_model = tf.keras.models.load_model(model_path)\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Register the custom loss function and load the model\n",
        "siamese_model = load_model('best_model_5k.h5', custom_objects={'contrastive_loss': contrastive_loss})\n",
        "\n",
        "# Suppose you have another processed audio tensor, reference_audio, to compare against\n",
        "# Here you should provide your model with both samples as a pair\n",
        "output = siamese_model([processed_padded, processed_padded_base])  # Assuming your model takes a list of two inputs\n",
        "\n",
        "# The output typically could be a similarity score or a classification result\n",
        "print(\"Model output:\", output.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JiJ-OrSy5JJ",
        "outputId": "1e765af3-d734-4849-c09b-9f663b2d502d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output: [[0.94970876]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gJuMbVdM3WBN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}