{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import audio_dataset_from_directory\n",
        "import numpy as np\n",
        "import librosa\n",
        "\n",
        "#tf.compat.v1.disable_eager_execution()\n",
        "#import tensorflow_io as tfio"
      ],
      "metadata": {
        "id": "Wn2kbYWMmA0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the input data from drive (eins, zwei, drei folders)\n",
        "!gdown 10P678fWDyAJIRv_HlqsXtS2u68NTFZ7I\n",
        "!unzip data_cnn.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2j6fp72mC9R",
        "outputId": "706c1362-8fbe-4cbf-960e-3075e24da639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=10P678fWDyAJIRv_HlqsXtS2u68NTFZ7I\n",
            "From (redirected): https://drive.google.com/uc?id=10P678fWDyAJIRv_HlqsXtS2u68NTFZ7I&confirm=t&uuid=22c5dcb2-3a2d-4047-acab-2cfc4c0e4745\n",
            "To: /content/data_cnn.zip\n",
            "100% 118M/118M [00:05<00:00, 23.3MB/s]\n",
            "Archive:  data_cnn.zip\n",
            "   creating: data_cnn/drei/\n",
            "  inflating: data_cnn/drei/Q1200603.wav  \n",
            "  inflating: data_cnn/drei/Q1200703.wav  \n",
            "  inflating: data_cnn/drei/Q1200803.wav  \n",
            "  inflating: data_cnn/drei/Q1200903.wav  \n",
            "  inflating: data_cnn/drei/Q1201103.wav  \n",
            "  inflating: data_cnn/drei/Q1201203.wav  \n",
            "  inflating: data_cnn/drei/Q1201303.wav  \n",
            "  inflating: data_cnn/drei/Q1201503.wav  \n",
            "  inflating: data_cnn/drei/Q1201703.wav  \n",
            "  inflating: data_cnn/drei/Q1201803.wav  \n",
            "  inflating: data_cnn/drei/Q1201903.wav  \n",
            "  inflating: data_cnn/drei/Q1202003.wav  \n",
            "  inflating: data_cnn/drei/Q1202103.wav  \n",
            "  inflating: data_cnn/drei/Q1202203.wav  \n",
            "  inflating: data_cnn/drei/Q1202303.wav  \n",
            "  inflating: data_cnn/drei/Q1202403.wav  \n",
            "  inflating: data_cnn/drei/Q1202503.wav  \n",
            "  inflating: data_cnn/drei/Q1202603.wav  \n",
            "  inflating: data_cnn/drei/Q1202703.wav  \n",
            "  inflating: data_cnn/drei/Q1202803.wav  \n",
            "  inflating: data_cnn/drei/Q1203003.wav  \n",
            "  inflating: data_cnn/drei/Q1203103.wav  \n",
            "  inflating: data_cnn/drei/Q1203203.wav  \n",
            "  inflating: data_cnn/drei/Q1203303.wav  \n",
            "  inflating: data_cnn/drei/Q1203603.wav  \n",
            "  inflating: data_cnn/drei/Q1203703.wav  \n",
            "  inflating: data_cnn/drei/Q1203803.wav  \n",
            "  inflating: data_cnn/drei/Q1203903.wav  \n",
            "  inflating: data_cnn/drei/Q1204203.wav  \n",
            "  inflating: data_cnn/drei/Q1204303.wav  \n",
            "  inflating: data_cnn/drei/Q1204403.wav  \n",
            "  inflating: data_cnn/drei/Q1204503.wav  \n",
            "  inflating: data_cnn/drei/Q1204603.wav  \n",
            "  inflating: data_cnn/drei/Q1204703.wav  \n",
            "  inflating: data_cnn/drei/Q1204803.wav  \n",
            "  inflating: data_cnn/drei/Q1204903.wav  \n",
            "  inflating: data_cnn/drei/Q1205003.wav  \n",
            "  inflating: data_cnn/drei/Q1205103.wav  \n",
            "  inflating: data_cnn/drei/Q1205203.wav  \n",
            "  inflating: data_cnn/drei/Q1205303.wav  \n",
            "  inflating: data_cnn/drei/Q1205403.wav  \n",
            "  inflating: data_cnn/drei/Q1205503.wav  \n",
            "  inflating: data_cnn/drei/Q1205603.wav  \n",
            "  inflating: data_cnn/drei/Q1205703.wav  \n",
            "  inflating: data_cnn/drei/Q1205803.wav  \n",
            "  inflating: data_cnn/drei/Q1205903.wav  \n",
            "  inflating: data_cnn/drei/Q1206003.wav  \n",
            "  inflating: data_cnn/drei/Q1206103.wav  \n",
            "  inflating: data_cnn/drei/Q1206203.wav  \n",
            "  inflating: data_cnn/drei/Q1206303.wav  \n",
            "  inflating: data_cnn/drei/Q1206403.wav  \n",
            "  inflating: data_cnn/drei/Q1206503.wav  \n",
            "  inflating: data_cnn/drei/Q1206603.wav  \n",
            "  inflating: data_cnn/drei/Q1206703.wav  \n",
            "  inflating: data_cnn/drei/Q1206803.wav  \n",
            "  inflating: data_cnn/drei/Q1206903.wav  \n",
            "  inflating: data_cnn/drei/Q1207003.wav  \n",
            "  inflating: data_cnn/drei/Q1207103.wav  \n",
            "  inflating: data_cnn/drei/Q1207203.wav  \n",
            "  inflating: data_cnn/drei/Q1207303.wav  \n",
            "  inflating: data_cnn/drei/Q1207403.wav  \n",
            "  inflating: data_cnn/drei/Q1207503.wav  \n",
            "  inflating: data_cnn/drei/Q1207603.wav  \n",
            "  inflating: data_cnn/drei/Q1207703.wav  \n",
            "  inflating: data_cnn/drei/Q1207803.wav  \n",
            "  inflating: data_cnn/drei/Q1207903.wav  \n",
            "  inflating: data_cnn/drei/Q1208003.wav  \n",
            "  inflating: data_cnn/drei/Q1208103.wav  \n",
            "  inflating: data_cnn/drei/Q1208203.wav  \n",
            "  inflating: data_cnn/drei/Q1208303.wav  \n",
            "  inflating: data_cnn/drei/Q1208403.wav  \n",
            "  inflating: data_cnn/drei/Q1208503.wav  \n",
            "  inflating: data_cnn/drei/Q1208603.wav  \n",
            "  inflating: data_cnn/drei/Q1208703.wav  \n",
            "  inflating: data_cnn/drei/Q1208903.wav  \n",
            "  inflating: data_cnn/drei/Q1209003.wav  \n",
            "  inflating: data_cnn/drei/Q1209103.wav  \n",
            "  inflating: data_cnn/drei/Q1209203.wav  \n",
            "  inflating: data_cnn/drei/Q1209303.wav  \n",
            "  inflating: data_cnn/drei/Q1209403.wav  \n",
            "  inflating: data_cnn/drei/Q1209503.wav  \n",
            "  inflating: data_cnn/drei/Q1209603.wav  \n",
            "  inflating: data_cnn/drei/Q1209703.wav  \n",
            "  inflating: data_cnn/drei/Q1209803.wav  \n",
            "  inflating: data_cnn/drei/Q1209903.wav  \n",
            "  inflating: data_cnn/drei/Q1210003.wav  \n",
            "  inflating: data_cnn/drei/Q1210203.wav  \n",
            "  inflating: data_cnn/drei/Q1300403.wav  \n",
            "  inflating: data_cnn/drei/Q1300603.wav  \n",
            "  inflating: data_cnn/drei/Q1300803.wav  \n",
            "  inflating: data_cnn/drei/Q1300903.wav  \n",
            "  inflating: data_cnn/drei/Q1301003.wav  \n",
            "  inflating: data_cnn/drei/Q1301203.wav  \n",
            "  inflating: data_cnn/drei/Q1301303.wav  \n",
            "  inflating: data_cnn/drei/Q1301403.wav  \n",
            "  inflating: data_cnn/drei/Q1301503.wav  \n",
            "  inflating: data_cnn/drei/Q1301603.wav  \n",
            "  inflating: data_cnn/drei/Q1301703.wav  \n",
            "  inflating: data_cnn/drei/Q1301803.wav  \n",
            "  inflating: data_cnn/drei/Q1301903.wav  \n",
            "  inflating: data_cnn/drei/Q1302103.wav  \n",
            "  inflating: data_cnn/drei/Q1302203.wav  \n",
            "  inflating: data_cnn/drei/Q1302303.wav  \n",
            "  inflating: data_cnn/drei/Q1302403.wav  \n",
            "  inflating: data_cnn/drei/Q1302603.wav  \n",
            "  inflating: data_cnn/drei/Q1302703.wav  \n",
            "  inflating: data_cnn/drei/Q1302803.wav  \n",
            "  inflating: data_cnn/drei/Q1302903.wav  \n",
            "  inflating: data_cnn/drei/Q1303003.wav  \n",
            "  inflating: data_cnn/drei/Q1303103.wav  \n",
            "  inflating: data_cnn/drei/Q1303203.wav  \n",
            "  inflating: data_cnn/drei/Q1303603.wav  \n",
            "  inflating: data_cnn/drei/Q1303703.wav  \n",
            "  inflating: data_cnn/drei/Q1303903.wav  \n",
            "  inflating: data_cnn/drei/Q1304003.wav  \n",
            "  inflating: data_cnn/drei/Q1304103.wav  \n",
            "  inflating: data_cnn/drei/Q1304303.wav  \n",
            "  inflating: data_cnn/drei/Q1304403.wav  \n",
            "  inflating: data_cnn/drei/Q1304503.wav  \n",
            "  inflating: data_cnn/drei/Q1304603.wav  \n",
            "  inflating: data_cnn/drei/Q1304703.wav  \n",
            "  inflating: data_cnn/drei/Q1304803.wav  \n",
            "  inflating: data_cnn/drei/Q1305003.wav  \n",
            "  inflating: data_cnn/drei/Q1305103.wav  \n",
            "  inflating: data_cnn/drei/Q1305303.wav  \n",
            "  inflating: data_cnn/drei/Q1305403.wav  \n",
            "  inflating: data_cnn/drei/Q1305503.wav  \n",
            "  inflating: data_cnn/drei/Q1305603.wav  \n",
            "  inflating: data_cnn/drei/Q1305703.wav  \n",
            "  inflating: data_cnn/drei/Q1305803.wav  \n",
            "  inflating: data_cnn/drei/Q1305903.wav  \n",
            "  inflating: data_cnn/drei/Q1306203.wav  \n",
            "  inflating: data_cnn/drei/Q1306303.wav  \n",
            "  inflating: data_cnn/drei/Q1306503.wav  \n",
            "  inflating: data_cnn/drei/Q1306603.wav  \n",
            "  inflating: data_cnn/drei/Q1306703.wav  \n",
            "  inflating: data_cnn/drei/Q1306803.wav  \n",
            "  inflating: data_cnn/drei/Q1306903.wav  \n",
            "  inflating: data_cnn/drei/Q1307003.wav  \n",
            "  inflating: data_cnn/drei/Q1307103.wav  \n",
            "  inflating: data_cnn/drei/Q1307203.wav  \n",
            "  inflating: data_cnn/drei/Q1307303.wav  \n",
            "  inflating: data_cnn/drei/Q1307403.wav  \n",
            "  inflating: data_cnn/drei/Q1307503.wav  \n",
            "  inflating: data_cnn/drei/Q1307603.wav  \n",
            "  inflating: data_cnn/drei/Q1307703.wav  \n",
            "  inflating: data_cnn/drei/Q1307803.wav  \n",
            "  inflating: data_cnn/drei/Q1307903.wav  \n",
            "  inflating: data_cnn/drei/Q1308003.wav  \n",
            "  inflating: data_cnn/drei/Q1308103.wav  \n",
            "  inflating: data_cnn/drei/Q1308203.wav  \n",
            "  inflating: data_cnn/drei/Q1308303.wav  \n",
            "  inflating: data_cnn/drei/Q1308403.wav  \n",
            "  inflating: data_cnn/drei/Q1308503.wav  \n",
            "  inflating: data_cnn/drei/Q1308603.wav  \n",
            "  inflating: data_cnn/drei/Q1308703.wav  \n",
            "  inflating: data_cnn/drei/Q1308803.wav  \n",
            "  inflating: data_cnn/drei/Q1308903.wav  \n",
            "  inflating: data_cnn/drei/Q1309003.wav  \n",
            "  inflating: data_cnn/drei/Q1309103.wav  \n",
            "  inflating: data_cnn/drei/Q1309203.wav  \n",
            "  inflating: data_cnn/drei/Q1309303.wav  \n",
            "  inflating: data_cnn/drei/Q1309403.wav  \n",
            "  inflating: data_cnn/drei/Q1309503.wav  \n",
            "  inflating: data_cnn/drei/Q1309603.wav  \n",
            "  inflating: data_cnn/drei/Q1309703.wav  \n",
            "  inflating: data_cnn/drei/Q1309803.wav  \n",
            "  inflating: data_cnn/drei/Q1309903.wav  \n",
            "  inflating: data_cnn/drei/Q1310003.wav  \n",
            "  inflating: data_cnn/drei/Q1310103.wav  \n",
            "  inflating: data_cnn/drei/Q1310203.wav  \n",
            "  inflating: data_cnn/drei/Q1310303.wav  \n",
            "  inflating: data_cnn/drei/Q1310403.wav  \n",
            "  inflating: data_cnn/drei/Q1310503.wav  \n",
            "   creating: data_cnn/eins/\n",
            "  inflating: data_cnn/eins/Q1200601.wav  \n",
            "  inflating: data_cnn/eins/Q1200701.wav  \n",
            "  inflating: data_cnn/eins/Q1200801.wav  \n",
            "  inflating: data_cnn/eins/Q1200901.wav  \n",
            "  inflating: data_cnn/eins/Q1201101.wav  \n",
            "  inflating: data_cnn/eins/Q1201201.wav  \n",
            "  inflating: data_cnn/eins/Q1201301.wav  \n",
            "  inflating: data_cnn/eins/Q1201501.wav  \n",
            "  inflating: data_cnn/eins/Q1201601.wav  \n",
            "  inflating: data_cnn/eins/Q1201701.wav  \n",
            "  inflating: data_cnn/eins/Q1201801.wav  \n",
            "  inflating: data_cnn/eins/Q1201901.wav  \n",
            "  inflating: data_cnn/eins/Q1202001.wav  \n",
            "  inflating: data_cnn/eins/Q1202101.wav  \n",
            "  inflating: data_cnn/eins/Q1202201.wav  \n",
            "  inflating: data_cnn/eins/Q1202301.wav  \n",
            "  inflating: data_cnn/eins/Q1202401.wav  \n",
            "  inflating: data_cnn/eins/Q1202501.wav  \n",
            "  inflating: data_cnn/eins/Q1202601.wav  \n",
            "  inflating: data_cnn/eins/Q1202701.wav  \n",
            "  inflating: data_cnn/eins/Q1202801.wav  \n",
            "  inflating: data_cnn/eins/Q1203001.wav  \n",
            "  inflating: data_cnn/eins/Q1203101.wav  \n",
            "  inflating: data_cnn/eins/Q1203201.wav  \n",
            "  inflating: data_cnn/eins/Q1203301.wav  \n",
            "  inflating: data_cnn/eins/Q1203601.wav  \n",
            "  inflating: data_cnn/eins/Q1203801.wav  \n",
            "  inflating: data_cnn/eins/Q1203901.wav  \n",
            "  inflating: data_cnn/eins/Q1204201.wav  \n",
            "  inflating: data_cnn/eins/Q1204301.wav  \n",
            "  inflating: data_cnn/eins/Q1204401.wav  \n",
            "  inflating: data_cnn/eins/Q1204501.wav  \n",
            "  inflating: data_cnn/eins/Q1204601.wav  \n",
            "  inflating: data_cnn/eins/Q1204701.wav  \n",
            "  inflating: data_cnn/eins/Q1204801.wav  \n",
            "  inflating: data_cnn/eins/Q1204901.wav  \n",
            "  inflating: data_cnn/eins/Q1205001.wav  \n",
            "  inflating: data_cnn/eins/Q1205101.wav  \n",
            "  inflating: data_cnn/eins/Q1205201.wav  \n",
            "  inflating: data_cnn/eins/Q1205301.wav  \n",
            "  inflating: data_cnn/eins/Q1205401.wav  \n",
            "  inflating: data_cnn/eins/Q1205501.wav  \n",
            "  inflating: data_cnn/eins/Q1205601.wav  \n",
            "  inflating: data_cnn/eins/Q1205701.wav  \n",
            "  inflating: data_cnn/eins/Q1205901.wav  \n",
            "  inflating: data_cnn/eins/Q1206001.wav  \n",
            "  inflating: data_cnn/eins/Q1206101.wav  \n",
            "  inflating: data_cnn/eins/Q1206201.wav  \n",
            "  inflating: data_cnn/eins/Q1206301.wav  \n",
            "  inflating: data_cnn/eins/Q1206401.wav  \n",
            "  inflating: data_cnn/eins/Q1206501.wav  \n",
            "  inflating: data_cnn/eins/Q1206601.wav  \n",
            "  inflating: data_cnn/eins/Q1206701.wav  \n",
            "  inflating: data_cnn/eins/Q1206801.wav  \n",
            "  inflating: data_cnn/eins/Q1206901.wav  \n",
            "  inflating: data_cnn/eins/Q1207001.wav  \n",
            "  inflating: data_cnn/eins/Q1207101.wav  \n",
            "  inflating: data_cnn/eins/Q1207201.wav  \n",
            "  inflating: data_cnn/eins/Q1207301.wav  \n",
            "  inflating: data_cnn/eins/Q1207401.wav  \n",
            "  inflating: data_cnn/eins/Q1207501.wav  \n",
            "  inflating: data_cnn/eins/Q1207601.wav  \n",
            "  inflating: data_cnn/eins/Q1207701.wav  \n",
            "  inflating: data_cnn/eins/Q1207801.wav  \n",
            "  inflating: data_cnn/eins/Q1207901.wav  \n",
            "  inflating: data_cnn/eins/Q1208001.wav  \n",
            "  inflating: data_cnn/eins/Q1208101.wav  \n",
            "  inflating: data_cnn/eins/Q1208201.wav  \n",
            "  inflating: data_cnn/eins/Q1208301.wav  \n",
            "  inflating: data_cnn/eins/Q1208501.wav  \n",
            "  inflating: data_cnn/eins/Q1208601.wav  \n",
            "  inflating: data_cnn/eins/Q1208701.wav  \n",
            "  inflating: data_cnn/eins/Q1208801.wav  \n",
            "  inflating: data_cnn/eins/Q1208901.wav  \n",
            "  inflating: data_cnn/eins/Q1209101.wav  \n",
            "  inflating: data_cnn/eins/Q1209201.wav  \n",
            "  inflating: data_cnn/eins/Q1209301.wav  \n",
            "  inflating: data_cnn/eins/Q1209401.wav  \n",
            "  inflating: data_cnn/eins/Q1209501.wav  \n",
            "  inflating: data_cnn/eins/Q1209601.wav  \n",
            "  inflating: data_cnn/eins/Q1209701.wav  \n",
            "  inflating: data_cnn/eins/Q1209801.wav  \n",
            "  inflating: data_cnn/eins/Q1209901.wav  \n",
            "  inflating: data_cnn/eins/Q1210001.wav  \n",
            "  inflating: data_cnn/eins/Q1210101.wav  \n",
            "  inflating: data_cnn/eins/Q1210201.wav  \n",
            "  inflating: data_cnn/eins/Q1300401.wav  \n",
            "  inflating: data_cnn/eins/Q1300601.wav  \n",
            "  inflating: data_cnn/eins/Q1300801.wav  \n",
            "  inflating: data_cnn/eins/Q1300901.wav  \n",
            "  inflating: data_cnn/eins/Q1301001.wav  \n",
            "  inflating: data_cnn/eins/Q1301201.wav  \n",
            "  inflating: data_cnn/eins/Q1301301.wav  \n",
            "  inflating: data_cnn/eins/Q1301401.wav  \n",
            "  inflating: data_cnn/eins/Q1301501.wav  \n",
            "  inflating: data_cnn/eins/Q1301701.wav  \n",
            "  inflating: data_cnn/eins/Q1301801.wav  \n",
            "  inflating: data_cnn/eins/Q1301901.wav  \n",
            "  inflating: data_cnn/eins/Q1302001.wav  \n",
            "  inflating: data_cnn/eins/Q1302101.wav  \n",
            "  inflating: data_cnn/eins/Q1302201.wav  \n",
            "  inflating: data_cnn/eins/Q1302301.wav  \n",
            "  inflating: data_cnn/eins/Q1302401.wav  \n",
            "  inflating: data_cnn/eins/Q1302601.wav  \n",
            "  inflating: data_cnn/eins/Q1302701.wav  \n",
            "  inflating: data_cnn/eins/Q1302801.wav  \n",
            "  inflating: data_cnn/eins/Q1302901.wav  \n",
            "  inflating: data_cnn/eins/Q1303001.wav  \n",
            "  inflating: data_cnn/eins/Q1303101.wav  \n",
            "  inflating: data_cnn/eins/Q1303201.wav  \n",
            "  inflating: data_cnn/eins/Q1303601.wav  \n",
            "  inflating: data_cnn/eins/Q1303701.wav  \n",
            "  inflating: data_cnn/eins/Q1303901.wav  \n",
            "  inflating: data_cnn/eins/Q1304001.wav  \n",
            "  inflating: data_cnn/eins/Q1304101.wav  \n",
            "  inflating: data_cnn/eins/Q1304301.wav  \n",
            "  inflating: data_cnn/eins/Q1304401.wav  \n",
            "  inflating: data_cnn/eins/Q1304501.wav  \n",
            "  inflating: data_cnn/eins/Q1304601.wav  \n",
            "  inflating: data_cnn/eins/Q1304701.wav  \n",
            "  inflating: data_cnn/eins/Q1304801.wav  \n",
            "  inflating: data_cnn/eins/Q1305001.wav  \n",
            "  inflating: data_cnn/eins/Q1305101.wav  \n",
            "  inflating: data_cnn/eins/Q1305301.wav  \n",
            "  inflating: data_cnn/eins/Q1305401.wav  \n",
            "  inflating: data_cnn/eins/Q1305501.wav  \n",
            "  inflating: data_cnn/eins/Q1305601.wav  \n",
            "  inflating: data_cnn/eins/Q1305701.wav  \n",
            "  inflating: data_cnn/eins/Q1305801.wav  \n",
            "  inflating: data_cnn/eins/Q1305901.wav  \n",
            "  inflating: data_cnn/eins/Q1306101.wav  \n",
            "  inflating: data_cnn/eins/Q1306201.wav  \n",
            "  inflating: data_cnn/eins/Q1306301.wav  \n",
            "  inflating: data_cnn/eins/Q1306401.wav  \n",
            "  inflating: data_cnn/eins/Q1306501.wav  \n",
            "  inflating: data_cnn/eins/Q1306601.wav  \n",
            "  inflating: data_cnn/eins/Q1306701.wav  \n",
            "  inflating: data_cnn/eins/Q1306801.wav  \n",
            "  inflating: data_cnn/eins/Q1306901.wav  \n",
            "  inflating: data_cnn/eins/Q1307001.wav  \n",
            "  inflating: data_cnn/eins/Q1307101.wav  \n",
            "  inflating: data_cnn/eins/Q1307201.wav  \n",
            "  inflating: data_cnn/eins/Q1307301.wav  \n",
            "  inflating: data_cnn/eins/Q1307401.wav  \n",
            "  inflating: data_cnn/eins/Q1307501.wav  \n",
            "  inflating: data_cnn/eins/Q1307601.wav  \n",
            "  inflating: data_cnn/eins/Q1307801.wav  \n",
            "  inflating: data_cnn/eins/Q1307901.wav  \n",
            "  inflating: data_cnn/eins/Q1308001.wav  \n",
            "  inflating: data_cnn/eins/Q1308101.wav  \n",
            "  inflating: data_cnn/eins/Q1308201.wav  \n",
            "  inflating: data_cnn/eins/Q1308301.wav  \n",
            "  inflating: data_cnn/eins/Q1308401.wav  \n",
            "  inflating: data_cnn/eins/Q1308501.wav  \n",
            "  inflating: data_cnn/eins/Q1308601.wav  \n",
            "  inflating: data_cnn/eins/Q1308701.wav  \n",
            "  inflating: data_cnn/eins/Q1308801.wav  \n",
            "  inflating: data_cnn/eins/Q1308901.wav  \n",
            "  inflating: data_cnn/eins/Q1309001.wav  \n",
            "  inflating: data_cnn/eins/Q1309101.wav  \n",
            "  inflating: data_cnn/eins/Q1309201.wav  \n",
            "  inflating: data_cnn/eins/Q1309301.wav  \n",
            "  inflating: data_cnn/eins/Q1309401.wav  \n",
            "  inflating: data_cnn/eins/Q1309501.wav  \n",
            "  inflating: data_cnn/eins/Q1309601.wav  \n",
            "  inflating: data_cnn/eins/Q1309701.wav  \n",
            "  inflating: data_cnn/eins/Q1309801.wav  \n",
            "  inflating: data_cnn/eins/Q1309901.wav  \n",
            "  inflating: data_cnn/eins/Q1310001.wav  \n",
            "  inflating: data_cnn/eins/Q1310101.wav  \n",
            "  inflating: data_cnn/eins/Q1310201.wav  \n",
            "  inflating: data_cnn/eins/Q1310301.wav  \n",
            "  inflating: data_cnn/eins/Q1310401.wav  \n",
            "  inflating: data_cnn/eins/Q1310501.wav  \n",
            "   creating: data_cnn/zwei/\n",
            "  inflating: data_cnn/zwei/Q1200602.wav  \n",
            "  inflating: data_cnn/zwei/Q1200702.wav  \n",
            "  inflating: data_cnn/zwei/Q1200802.wav  \n",
            "  inflating: data_cnn/zwei/Q1200902.wav  \n",
            "  inflating: data_cnn/zwei/Q1201102.wav  \n",
            "  inflating: data_cnn/zwei/Q1201202.wav  \n",
            "  inflating: data_cnn/zwei/Q1201302.wav  \n",
            "  inflating: data_cnn/zwei/Q1201502.wav  \n",
            "  inflating: data_cnn/zwei/Q1201602.wav  \n",
            "  inflating: data_cnn/zwei/Q1201702.wav  \n",
            "  inflating: data_cnn/zwei/Q1201802.wav  \n",
            "  inflating: data_cnn/zwei/Q1201902.wav  \n",
            "  inflating: data_cnn/zwei/Q1202002.wav  \n",
            "  inflating: data_cnn/zwei/Q1202102.wav  \n",
            "  inflating: data_cnn/zwei/Q1202202.wav  \n",
            "  inflating: data_cnn/zwei/Q1202302.wav  \n",
            "  inflating: data_cnn/zwei/Q1202402.wav  \n",
            "  inflating: data_cnn/zwei/Q1202502.wav  \n",
            "  inflating: data_cnn/zwei/Q1202602.wav  \n",
            "  inflating: data_cnn/zwei/Q1202702.wav  \n",
            "  inflating: data_cnn/zwei/Q1202802.wav  \n",
            "  inflating: data_cnn/zwei/Q1202902.wav  \n",
            "  inflating: data_cnn/zwei/Q1203002.wav  \n",
            "  inflating: data_cnn/zwei/Q1203102.wav  \n",
            "  inflating: data_cnn/zwei/Q1203202.wav  \n",
            "  inflating: data_cnn/zwei/Q1203302.wav  \n",
            "  inflating: data_cnn/zwei/Q1203602.wav  \n",
            "  inflating: data_cnn/zwei/Q1203702.wav  \n",
            "  inflating: data_cnn/zwei/Q1203802.wav  \n",
            "  inflating: data_cnn/zwei/Q1203902.wav  \n",
            "  inflating: data_cnn/zwei/Q1204202.wav  \n",
            "  inflating: data_cnn/zwei/Q1204302.wav  \n",
            "  inflating: data_cnn/zwei/Q1204402.wav  \n",
            "  inflating: data_cnn/zwei/Q1204502.wav  \n",
            "  inflating: data_cnn/zwei/Q1204602.wav  \n",
            "  inflating: data_cnn/zwei/Q1204702.wav  \n",
            "  inflating: data_cnn/zwei/Q1204802.wav  \n",
            "  inflating: data_cnn/zwei/Q1204902.wav  \n",
            "  inflating: data_cnn/zwei/Q1205002.wav  \n",
            "  inflating: data_cnn/zwei/Q1205102.wav  \n",
            "  inflating: data_cnn/zwei/Q1205202.wav  \n",
            "  inflating: data_cnn/zwei/Q1205302.wav  \n",
            "  inflating: data_cnn/zwei/Q1205402.wav  \n",
            "  inflating: data_cnn/zwei/Q1205502.wav  \n",
            "  inflating: data_cnn/zwei/Q1205602.wav  \n",
            "  inflating: data_cnn/zwei/Q1205702.wav  \n",
            "  inflating: data_cnn/zwei/Q1205802.wav  \n",
            "  inflating: data_cnn/zwei/Q1205902.wav  \n",
            "  inflating: data_cnn/zwei/Q1206002.wav  \n",
            "  inflating: data_cnn/zwei/Q1206102.wav  \n",
            "  inflating: data_cnn/zwei/Q1206202.wav  \n",
            "  inflating: data_cnn/zwei/Q1206402.wav  \n",
            "  inflating: data_cnn/zwei/Q1206502.wav  \n",
            "  inflating: data_cnn/zwei/Q1206602.wav  \n",
            "  inflating: data_cnn/zwei/Q1206702.wav  \n",
            "  inflating: data_cnn/zwei/Q1206802.wav  \n",
            "  inflating: data_cnn/zwei/Q1206902.wav  \n",
            "  inflating: data_cnn/zwei/Q1207002.wav  \n",
            "  inflating: data_cnn/zwei/Q1207102.wav  \n",
            "  inflating: data_cnn/zwei/Q1207202.wav  \n",
            "  inflating: data_cnn/zwei/Q1207302.wav  \n",
            "  inflating: data_cnn/zwei/Q1207402.wav  \n",
            "  inflating: data_cnn/zwei/Q1207502.wav  \n",
            "  inflating: data_cnn/zwei/Q1207602.wav  \n",
            "  inflating: data_cnn/zwei/Q1207702.wav  \n",
            "  inflating: data_cnn/zwei/Q1207802.wav  \n",
            "  inflating: data_cnn/zwei/Q1207902.wav  \n",
            "  inflating: data_cnn/zwei/Q1208002.wav  \n",
            "  inflating: data_cnn/zwei/Q1208102.wav  \n",
            "  inflating: data_cnn/zwei/Q1208202.wav  \n",
            "  inflating: data_cnn/zwei/Q1208302.wav  \n",
            "  inflating: data_cnn/zwei/Q1208402.wav  \n",
            "  inflating: data_cnn/zwei/Q1208502.wav  \n",
            "  inflating: data_cnn/zwei/Q1208602.wav  \n",
            "  inflating: data_cnn/zwei/Q1208702.wav  \n",
            "  inflating: data_cnn/zwei/Q1208802.wav  \n",
            "  inflating: data_cnn/zwei/Q1208902.wav  \n",
            "  inflating: data_cnn/zwei/Q1209002.wav  \n",
            "  inflating: data_cnn/zwei/Q1209102.wav  \n",
            "  inflating: data_cnn/zwei/Q1209202.wav  \n",
            "  inflating: data_cnn/zwei/Q1209302.wav  \n",
            "  inflating: data_cnn/zwei/Q1209402.wav  \n",
            "  inflating: data_cnn/zwei/Q1209502.wav  \n",
            "  inflating: data_cnn/zwei/Q1209602.wav  \n",
            "  inflating: data_cnn/zwei/Q1209702.wav  \n",
            "  inflating: data_cnn/zwei/Q1209802.wav  \n",
            "  inflating: data_cnn/zwei/Q1209902.wav  \n",
            "  inflating: data_cnn/zwei/Q1210002.wav  \n",
            "  inflating: data_cnn/zwei/Q1210202.wav  \n",
            "  inflating: data_cnn/zwei/Q1300402.wav  \n",
            "  inflating: data_cnn/zwei/Q1300602.wav  \n",
            "  inflating: data_cnn/zwei/Q1300802.wav  \n",
            "  inflating: data_cnn/zwei/Q1300902.wav  \n",
            "  inflating: data_cnn/zwei/Q1301002.wav  \n",
            "  inflating: data_cnn/zwei/Q1301202.wav  \n",
            "  inflating: data_cnn/zwei/Q1301302.wav  \n",
            "  inflating: data_cnn/zwei/Q1301402.wav  \n",
            "  inflating: data_cnn/zwei/Q1301502.wav  \n",
            "  inflating: data_cnn/zwei/Q1301602.wav  \n",
            "  inflating: data_cnn/zwei/Q1301702.wav  \n",
            "  inflating: data_cnn/zwei/Q1301802.wav  \n",
            "  inflating: data_cnn/zwei/Q1301902.wav  \n",
            "  inflating: data_cnn/zwei/Q1302002.wav  \n",
            "  inflating: data_cnn/zwei/Q1302102.wav  \n",
            "  inflating: data_cnn/zwei/Q1302202.wav  \n",
            "  inflating: data_cnn/zwei/Q1302302.wav  \n",
            "  inflating: data_cnn/zwei/Q1302402.wav  \n",
            "  inflating: data_cnn/zwei/Q1302602.wav  \n",
            "  inflating: data_cnn/zwei/Q1302702.wav  \n",
            "  inflating: data_cnn/zwei/Q1302802.wav  \n",
            "  inflating: data_cnn/zwei/Q1302902.wav  \n",
            "  inflating: data_cnn/zwei/Q1303002.wav  \n",
            "  inflating: data_cnn/zwei/Q1303102.wav  \n",
            "  inflating: data_cnn/zwei/Q1303202.wav  \n",
            "  inflating: data_cnn/zwei/Q1303602.wav  \n",
            "  inflating: data_cnn/zwei/Q1303702.wav  \n",
            "  inflating: data_cnn/zwei/Q1303802.wav  \n",
            "  inflating: data_cnn/zwei/Q1303902.wav  \n",
            "  inflating: data_cnn/zwei/Q1304002.wav  \n",
            "  inflating: data_cnn/zwei/Q1304102.wav  \n",
            "  inflating: data_cnn/zwei/Q1304302.wav  \n",
            "  inflating: data_cnn/zwei/Q1304402.wav  \n",
            "  inflating: data_cnn/zwei/Q1304502.wav  \n",
            "  inflating: data_cnn/zwei/Q1304602.wav  \n",
            "  inflating: data_cnn/zwei/Q1304702.wav  \n",
            "  inflating: data_cnn/zwei/Q1304802.wav  \n",
            "  inflating: data_cnn/zwei/Q1305002.wav  \n",
            "  inflating: data_cnn/zwei/Q1305102.wav  \n",
            "  inflating: data_cnn/zwei/Q1305302.wav  \n",
            "  inflating: data_cnn/zwei/Q1305402.wav  \n",
            "  inflating: data_cnn/zwei/Q1305502.wav  \n",
            "  inflating: data_cnn/zwei/Q1305602.wav  \n",
            "  inflating: data_cnn/zwei/Q1305702.wav  \n",
            "  inflating: data_cnn/zwei/Q1305802.wav  \n",
            "  inflating: data_cnn/zwei/Q1305902.wav  \n",
            "  inflating: data_cnn/zwei/Q1306102.wav  \n",
            "  inflating: data_cnn/zwei/Q1306202.wav  \n",
            "  inflating: data_cnn/zwei/Q1306302.wav  \n",
            "  inflating: data_cnn/zwei/Q1306402.wav  \n",
            "  inflating: data_cnn/zwei/Q1306502.wav  \n",
            "  inflating: data_cnn/zwei/Q1306602.wav  \n",
            "  inflating: data_cnn/zwei/Q1306702.wav  \n",
            "  inflating: data_cnn/zwei/Q1306802.wav  \n",
            "  inflating: data_cnn/zwei/Q1306902.wav  \n",
            "  inflating: data_cnn/zwei/Q1307002.wav  \n",
            "  inflating: data_cnn/zwei/Q1307102.wav  \n",
            "  inflating: data_cnn/zwei/Q1307202.wav  \n",
            "  inflating: data_cnn/zwei/Q1307302.wav  \n",
            "  inflating: data_cnn/zwei/Q1307402.wav  \n",
            "  inflating: data_cnn/zwei/Q1307502.wav  \n",
            "  inflating: data_cnn/zwei/Q1307602.wav  \n",
            "  inflating: data_cnn/zwei/Q1307702.wav  \n",
            "  inflating: data_cnn/zwei/Q1307802.wav  \n",
            "  inflating: data_cnn/zwei/Q1307902.wav  \n",
            "  inflating: data_cnn/zwei/Q1308002.wav  \n",
            "  inflating: data_cnn/zwei/Q1308102.wav  \n",
            "  inflating: data_cnn/zwei/Q1308202.wav  \n",
            "  inflating: data_cnn/zwei/Q1308302.wav  \n",
            "  inflating: data_cnn/zwei/Q1308402.wav  \n",
            "  inflating: data_cnn/zwei/Q1308502.wav  \n",
            "  inflating: data_cnn/zwei/Q1308602.wav  \n",
            "  inflating: data_cnn/zwei/Q1308702.wav  \n",
            "  inflating: data_cnn/zwei/Q1308902.wav  \n",
            "  inflating: data_cnn/zwei/Q1309002.wav  \n",
            "  inflating: data_cnn/zwei/Q1309102.wav  \n",
            "  inflating: data_cnn/zwei/Q1309202.wav  \n",
            "  inflating: data_cnn/zwei/Q1309302.wav  \n",
            "  inflating: data_cnn/zwei/Q1309402.wav  \n",
            "  inflating: data_cnn/zwei/Q1309502.wav  \n",
            "  inflating: data_cnn/zwei/Q1309602.wav  \n",
            "  inflating: data_cnn/zwei/Q1309702.wav  \n",
            "  inflating: data_cnn/zwei/Q1309902.wav  \n",
            "  inflating: data_cnn/zwei/Q1310002.wav  \n",
            "  inflating: data_cnn/zwei/Q1310102.wav  \n",
            "  inflating: data_cnn/zwei/Q1310202.wav  \n",
            "  inflating: data_cnn/zwei/Q1310302.wav  \n",
            "  inflating: data_cnn/zwei/Q1310402.wav  \n",
            "  inflating: data_cnn/zwei/Q1310502.wav  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_dir = \"/content/data_cnn\"\n",
        "\n",
        "train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
        "    directory=data_dir,\n",
        "    batch_size=None,\n",
        "    validation_split=0.2,\n",
        "    seed=0,\n",
        "    subset='both')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qBBqNe7mFqc",
        "outputId": "6453fca4-3e8f-4ea7-d5a4-acf3defc3937"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 526 files belonging to 3 classes.\n",
            "Using 421 files for training.\n",
            "Using 105 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_audio_mfps(audio, label):\n",
        "    # Convert audio tensor to a compatible format\n",
        "    audio = tf.cast(audio, tf.float32)  # Cast audio to float32\n",
        "    audio = audio / 32768.0  # Normalize audio\n",
        "\n",
        "    # Extract mel-frequency power spectra\n",
        "    def _extract_mel(audio):\n",
        "        # Reshape the audio tensor to (batch_size, num_samples) as expected by tf.signal.stft\n",
        "        audio = tf.reshape(audio, [-1])\n",
        "\n",
        "        # Compute mel-frequency power spectra\n",
        "        stfts = tf.signal.stft(audio, frame_length=1024, frame_step=512, fft_length=1024)\n",
        "        spectrograms = tf.abs(stfts)\n",
        "\n",
        "        num_spectrogram_bins = stfts.shape[-1]\n",
        "        lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 128\n",
        "        linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
        "            num_mel_bins, num_spectrogram_bins, 16000, lower_edge_hertz, upper_edge_hertz)\n",
        "\n",
        "        mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\n",
        "\n",
        "        return mel_spectrograms\n",
        "\n",
        "    # Use tf.py_function to call _extract_mel with audio tensor\n",
        "    mel_spectra = tf.py_function(_extract_mel, [audio], tf.float32)\n",
        "\n",
        "    return mel_spectra, label"
      ],
      "metadata": {
        "id": "kIGDCiX7mI0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_processed_mspec = train_ds.map(preprocess_audio_mfps)\n",
        "val_processed_mspec = val_ds.map(preprocess_audio_mfps)"
      ],
      "metadata": {
        "id": "ymmKHYs6mLV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Find the maximum sequence length in the training dataset\n",
        "max_length = max(len(seq) for seq, _ in train_processed_mspec.as_numpy_iterator())\n",
        "\n",
        "# Function to pad sequences\n",
        "def pad_sequence(seq, label):\n",
        "    padded_seq = tf.pad(seq, paddings=[[0, max_length - tf.shape(seq)[0]], [0, 0]])\n",
        "    return padded_seq, label\n",
        "\n",
        "# Pad the training dataset\n",
        "padded_train_ds_mfps = train_processed_mspec.map(pad_sequence)\n",
        "\n",
        "# Pad the validation dataset\n",
        "padded_val_ds_mfps = val_processed_mspec.map(pad_sequence)"
      ],
      "metadata": {
        "id": "trTA591nmM_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_pairs_and_labels(padded_dataset):\n",
        "    pairs = []\n",
        "    labels = []\n",
        "\n",
        "    for sequence, label in padded_dataset:\n",
        "        # Assuming 'sequence' is your padded sequence and 'label' is its corresponding label\n",
        "        pairs.append(sequence)\n",
        "        labels.append(label)\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    pairs = np.array(pairs)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    return pairs, labels\n",
        "\n",
        "# Apply the function to your padded datasets\n",
        "train_data, train_labels = create_pairs_and_labels(padded_train_ds_mfps)\n",
        "val_data, val_labels = create_pairs_and_labels(padded_val_ds_mfps)\n"
      ],
      "metadata": {
        "id": "GO58mXGzmOtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have your dataset X and corresponding labels y\n",
        "# X.shape = (num_samples, input_vector_size)\n",
        "# y.shape = (num_samples,)\n",
        "\n",
        "# Function to create pairs of data and labels\n",
        "def create_pairs(X, y, num_pairs):\n",
        "    pairs = []\n",
        "    labels = []\n",
        "    num_classes = len(np.unique(y))\n",
        "    class_indices = [np.where(y == i)[0] for i in range(num_classes)]\n",
        "\n",
        "    for _ in range(num_pairs):\n",
        "        # Select a random class (label)\n",
        "        class_idx = np.random.randint(0, num_classes)\n",
        "        # Select a random sample from the selected class\n",
        "        idx_1 = np.random.choice(class_indices[class_idx])\n",
        "        # Ensure that the second sample is from the same class for half of the pairs\n",
        "        should_be_same_class = np.random.randint(0, 2)\n",
        "        if should_be_same_class:\n",
        "            idx_2 = np.random.choice(class_indices[class_idx])\n",
        "        else:\n",
        "            # Select a random class different from the first one\n",
        "            class_idx_2 = (class_idx + np.random.randint(1, num_classes)) % num_classes\n",
        "            idx_2 = np.random.choice(class_indices[class_idx_2])\n",
        "        pairs.append([X[idx_1], X[idx_2]])\n",
        "        # 1 if same class, 0 if different class\n",
        "        labels.append(1 if should_be_same_class else 0)\n",
        "\n",
        "    return np.array(pairs), np.array(labels)\n",
        "\n",
        "# Example usage:\n",
        "num_pairs = 1500  # Adjust this number based on your dataset size and requirements\n",
        "train_pairs, train_pairs_labels = create_pairs(train_data, train_labels, num_pairs)\n",
        "num_pairs_val = 500\n",
        "val_pairs, val_pairs_labels = create_pairs(val_data, val_labels, num_pairs_val)"
      ],
      "metadata": {
        "id": "uHkixqWymQrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert labels to integers\n",
        "train_pairs_labels = train_pairs_labels.astype(np.float32)\n",
        "val_pairs_labels = val_pairs_labels.astype(np.float32)\n"
      ],
      "metadata": {
        "id": "cbh9pINWpxAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv1D, BatchNormalization, MaxPooling1D, GlobalAveragePooling1D, Lambda\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
        "import keras.backend as K\n",
        "\n",
        "# Define the Siamese Network model architecture with variable layers, filters, and kernel size\n",
        "def siamese_model(params):\n",
        "    input_shape = (402, 128)  # Define the shape of your input vectors\n",
        "    model_input = Input(shape=input_shape)\n",
        "    x = model_input\n",
        "\n",
        "    # Dynamically adding layers based on num_layers\n",
        "    for i in range(params['num_layers']):\n",
        "        x = Conv1D(params['num_filters'], kernel_size=params['kernel_size'], activation='relu', padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        if i < params['num_layers'] - 1:  # add max pooling to all but the last layer\n",
        "            x = MaxPooling1D(pool_size=2, strides=2)(x)\n",
        "\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    return Model(inputs=model_input, outputs=x)\n",
        "\n",
        "# Define cosine similarity function\n",
        "def cosine_similarity(vectors):\n",
        "    x, y = vectors\n",
        "    x = K.l2_normalize(x, axis=-1)\n",
        "    y = K.l2_normalize(y, axis=-1)\n",
        "    return K.sum(x * y, axis=-1, keepdims=True)\n",
        "\n",
        "# Define contrastive loss function\n",
        "def contrastive_loss(y_true, y_pred):\n",
        "    margin = 1.0\n",
        "    return K.mean(y_true * K.square(1 - y_pred) + (1 - y_true) * K.square(K.maximum(y_pred - margin, 0)))\n",
        "\n",
        "# Define the objective function for hyperparameter optimization\n",
        "def objective(params):\n",
        "    # Create the Siamese model with current params\n",
        "    base_model = siamese_model(params)\n",
        "\n",
        "    # Inputs of the Siamese Network\n",
        "    input_a = Input(shape=(402, 128))\n",
        "    input_b = Input(shape=(402, 128))\n",
        "\n",
        "    # Process both inputs through the same base model\n",
        "    processed_a = base_model(input_a)\n",
        "    processed_b = base_model(input_b)\n",
        "\n",
        "    # Calculate the cosine similarity\n",
        "    cosine_sim = Lambda(cosine_similarity, output_shape=(1,))([processed_a, processed_b])\n",
        "\n",
        "    # Final Siamese network model\n",
        "    siamese_network = Model(inputs=[input_a, input_b], outputs=cosine_sim)\n",
        "\n",
        "    # Select and configure the optimizer\n",
        "    if params['optimizer']['type'] == 'adam':\n",
        "        optimizer = Adam(learning_rate=params['optimizer']['lr'])\n",
        "    elif params['optimizer']['type'] == 'sgd':\n",
        "        optimizer = SGD(learning_rate=params['optimizer']['lr'], momentum=params['optimizer']['momentum'])\n",
        "    elif params['optimizer']['type'] == 'rmsprop':\n",
        "        optimizer = RMSprop(learning_rate=params['optimizer']['lr'])\n",
        "\n",
        "    siamese_network.compile(optimizer=optimizer, loss=contrastive_loss)\n",
        "\n",
        "    # Dummy data placeholders, replace with actual data\n",
        "    x1_train, x2_train, y_train, x1_val, x2_val, y_val =train_pairs[:, 0], train_pairs[:, 1], train_pairs_labels,val_pairs[:, 0], val_pairs[:, 1], val_pairs_labels\n",
        "\n",
        "    # Training (assuming dummy placeholders are replaced)\n",
        "    result = siamese_network.fit([x1_train, x2_train], y_train, epochs=10, batch_size=params['batch_size'], validation_data=([x1_val, x2_val], y_val), verbose=0)\n",
        "    validation_loss = np.min(result.history['val_loss'])\n",
        "    return {'loss': validation_loss, 'status': STATUS_OK}\n",
        "\n",
        "# Define the search space for hyperparameters\n",
        "space = {\n",
        "    'num_layers': hp.choice('num_layers', range(3, 7)),  # Reduced range for simplicity\n",
        "    'num_filters': hp.choice('num_filters', [16, 32, 64, 128, 256]),\n",
        "    'kernel_size': hp.choice('kernel_size', [3, 5, 7]),\n",
        "    'batch_size': hp.choice('batch_size', [16, 32, 64]),\n",
        "    'optimizer': hp.choice('optimizer', [\n",
        "        {'type': 'adam', 'lr': hp.loguniform('adam_lr', np.log(1e-4), np.log(1e-2))},\n",
        "        {'type': 'sgd', 'lr': hp.loguniform('sgd_lr', np.log(1e-4), np.log(1e-2)), 'momentum': hp.uniform('sgd_momentum', 0.0, 0.99)},\n",
        "        {'type': 'rmsprop', 'lr': hp.loguniform('rmsprop_lr', np.log(1e-4), np.log(1e-2))}\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Running the hyperparameter optimization\n",
        "trials = Trials()\n",
        "best_params = fmin(\n",
        "    fn=objective,\n",
        "    space=space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=10,  # Adjust based on computational resources and needs\n",
        "    trials=trials\n",
        ")\n",
        "\n",
        "print(\"Best parameters:\", best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiz320sUmUn7",
        "outputId": "d95a00de-8cbe-4759-c3c5-265a47c26e77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 10/10 [04:08<00:00, 24.82s/trial, best loss: 1.4637180373598723e-15]\n",
            "Best parameters: {'batch_size': 1, 'kernel_size': 1, 'num_filters': 3, 'num_layers': 2, 'optimizer': 1, 'sgd_lr': 0.00015588365602147455, 'sgd_momentum': 0.8494444660753903}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After finding the best parameters, decode them to their actual meanings\n",
        "def decode_hyperparameters(params):\n",
        "    return {\n",
        "        'num_layers': range(3, 7)[params['num_layers']],\n",
        "        'num_filters': [16, 32, 64, 128, 256, 512][params['num_filters']],\n",
        "        'kernel_size': [3, 5, 7][params['kernel_size']],\n",
        "        'batch_size': [16, 32, 64][params['batch_size']]\n",
        "        #'optimizer': ['adam', 'sgd', 'rmsprop'][params['optimizer']['type']],'lr': params['optimizer']['lr']  # This will need proper mapping if it's also indexed\n",
        "    }\n",
        "\n",
        "decoded_params = decode_hyperparameters(best_params)\n",
        "print(\"Decoded parameters:\", decoded_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMYt1JJz2Hp_",
        "outputId": "ca878be2-61e4-4caa-b252-d185c699a015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoded parameters: {'num_layers': 5, 'num_filters': 128, 'kernel_size': 5, 'batch_size': 32}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv1D, BatchNormalization, MaxPooling1D, GlobalAveragePooling1D, Lambda\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
        "import keras.backend as K\n",
        "\n",
        "# Define the Siamese Network model architecture with variable layers, filters, and kernel size\n",
        "def siamese_model(params):\n",
        "    input_shape = (402, 128)  # Define the shape of your input vectors\n",
        "    model_input = Input(shape=input_shape)\n",
        "    x = model_input\n",
        "\n",
        "    # Dynamically adding layers based on num_layers\n",
        "    for i in range(params['num_layers']):\n",
        "        x = Conv1D(params['num_filters'], kernel_size=params['kernel_size'], activation='relu', padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        if i < params['num_layers'] - 1:  # add max pooling to all but the last layer\n",
        "            x = MaxPooling1D(pool_size=2, strides=2)(x)\n",
        "\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    return Model(inputs=model_input, outputs=x)\n",
        "\n",
        "# Define cosine similarity function\n",
        "def cosine_similarity(vectors):\n",
        "    x, y = vectors\n",
        "    x = K.l2_normalize(x, axis=-1)\n",
        "    y = K.l2_normalize(y, axis=-1)\n",
        "    return K.sum(x * y, axis=-1, keepdims=True)\n",
        "\n",
        "# Define contrastive loss function\n",
        "def contrastive_loss(y_true, y_pred):\n",
        "    margin = 1.0\n",
        "    return K.mean(y_true * K.square(1 - y_pred) + (1 - y_true) * K.square(K.maximum(y_pred - margin, 0)))\n",
        "\n",
        "# Define the objective function for hyperparameter optimization\n",
        "def objective(params):\n",
        "    # Create the Siamese model with current params\n",
        "    base_model = siamese_model(params)\n",
        "\n",
        "    # Inputs of the Siamese Network\n",
        "    input_a = Input(shape=(402, 128))\n",
        "    input_b = Input(shape=(402, 128))\n",
        "\n",
        "    # Process both inputs through the same base model\n",
        "    processed_a = base_model(input_a)\n",
        "    processed_b = base_model(input_b)\n",
        "\n",
        "    # Calculate the cosine similarity\n",
        "    cosine_sim = Lambda(cosine_similarity, output_shape=(1,))([processed_a, processed_b])\n",
        "\n",
        "    # Final Siamese network model\n",
        "    siamese_network = Model(inputs=[input_a, input_b], outputs=cosine_sim)\n",
        "\n",
        "    # Select and configure the optimizer\n",
        "    if params['optimizer']['type'] == 'adam':\n",
        "        optimizer = Adam(learning_rate=params['optimizer']['lr'])\n",
        "    elif params['optimizer']['type'] == 'sgd':\n",
        "        optimizer = SGD(learning_rate=params['optimizer']['lr'], momentum=params['optimizer']['momentum'])\n",
        "    elif params['optimizer']['type'] == 'rmsprop':\n",
        "        optimizer = RMSprop(learning_rate=params['optimizer']['lr'])\n",
        "\n",
        "    siamese_network.compile(optimizer=optimizer, loss=contrastive_loss)\n",
        "\n",
        "    # Dummy data placeholders, replace with actual data\n",
        "    x1_train, x2_train, y_train, x1_val, x2_val, y_val =train_pairs[:, 0], train_pairs[:, 1], train_pairs_labels,val_pairs[:, 0], val_pairs[:, 1], val_pairs_labels\n",
        "\n",
        "    # Training (assuming dummy placeholders are replaced)\n",
        "    result = siamese_network.fit([x1_train, x2_train], y_train, epochs=10, batch_size=params['batch_size'], validation_data=([x1_val, x2_val], y_val), verbose=0)\n",
        "    validation_loss = np.min(result.history['val_loss'])\n",
        "    return {'loss': validation_loss, 'status': STATUS_OK}\n",
        "\n",
        "# Define the search space for hyperparameters\n",
        "space = {\n",
        "    'num_layers': hp.choice('num_layers', range(3, 7)),  # Reduced range for simplicity\n",
        "    'num_filters': hp.choice('num_filters', [16, 32, 64, 128, 256]),\n",
        "    'kernel_size': hp.choice('kernel_size', [3, 5, 7]),\n",
        "    'batch_size': hp.choice('batch_size', [16, 32, 64]),\n",
        "    'optimizer': hp.choice('optimizer', [\n",
        "        {'type': 'adam', 'lr': hp.loguniform('adam_lr', np.log(1e-4), np.log(1e-2))},\n",
        "        {'type': 'sgd', 'lr': hp.loguniform('sgd_lr', np.log(1e-4), np.log(1e-2)), 'momentum': hp.uniform('sgd_momentum', 0.0, 0.99)},\n",
        "        {'type': 'rmsprop', 'lr': hp.loguniform('rmsprop_lr', np.log(1e-4), np.log(1e-2))}\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Running the hyperparameter optimization\n",
        "trials = Trials()\n",
        "best_params = fmin(\n",
        "    fn=objective,\n",
        "    space=space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=25,  # Adjust based on computational resources and needs\n",
        "    trials=trials\n",
        ")\n",
        "\n",
        "print(\"Best parameters:\", best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YugtDUxsWSP",
        "outputId": "a4e77bed-9b4a-4b31-f62b-b2027d8d6873"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 25/25 [09:33<00:00, 22.94s/trial, best loss: 1.8900436957565913e-15]\n",
            "Best parameters: {'batch_size': 2, 'kernel_size': 2, 'num_filters': 2, 'num_layers': 3, 'optimizer': 1, 'sgd_lr': 0.00010112553577170664, 'sgd_momentum': 0.308640937804029}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After finding the best parameters, decode them to their actual meanings\n",
        "def decode_hyperparameters(params):\n",
        "    return {\n",
        "        'num_layers': range(3, 7)[params['num_layers']],\n",
        "        'num_filters': [16, 32, 64, 128, 256, 512][params['num_filters']],\n",
        "        'kernel_size': [3, 5, 7][params['kernel_size']],\n",
        "        'batch_size': [16, 32, 64][params['batch_size']],\n",
        "        #'optimizer': ['adam', 'sgd', 'rmsprop'][params['optimizer']['type']],\n",
        "       # 'lr': params['optimizer']['lr']  # This will need proper mapping if it's also indexed\n",
        "    }\n",
        "\n",
        "decoded_params = decode_hyperparameters(best_params)\n",
        "print(\"Decoded parameters:\", decoded_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sTS9-qfpx36",
        "outputId": "daa124a1-a01c-4080-be23-34399517f7d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoded parameters: {'num_layers': 6, 'num_filters': 64, 'kernel_size': 7, 'batch_size': 64}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv1D, BatchNormalization, MaxPooling1D, GlobalAveragePooling1D, Lambda\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
        "import keras.backend as K\n",
        "\n",
        "# Define the Siamese Network model architecture with variable layers, filters, and kernel size\n",
        "def siamese_model(params):\n",
        "    input_shape = (402, 128)  # Define the shape of your input vectors\n",
        "    model_input = Input(shape=input_shape)\n",
        "    x = model_input\n",
        "\n",
        "    # Dynamically adding layers based on num_layers\n",
        "    for i in range(params['num_layers']):\n",
        "        x = Conv1D(params['num_filters'], kernel_size=params['kernel_size'], activation='relu', padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        if i < params['num_layers'] - 1:  # add max pooling to all but the last layer\n",
        "            x = MaxPooling1D(pool_size=2, strides=2)(x)\n",
        "\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    return Model(inputs=model_input, outputs=x)\n",
        "\n",
        "# Define cosine similarity function\n",
        "def cosine_similarity(vectors):\n",
        "    x, y = vectors\n",
        "    x = K.l2_normalize(x, axis=-1)\n",
        "    y = K.l2_normalize(y, axis=-1)\n",
        "    return K.sum(x * y, axis=-1, keepdims=True)\n",
        "\n",
        "# Define contrastive loss function\n",
        "def contrastive_loss(y_true, y_pred):\n",
        "    margin = 1.0\n",
        "    return K.mean(y_true * K.square(1 - y_pred) + (1 - y_true) * K.square(K.maximum(y_pred - margin, 0)))\n",
        "\n",
        "# Define the objective function for hyperparameter optimization\n",
        "def objective(params):\n",
        "    # Create the Siamese model with current params\n",
        "    base_model = siamese_model(params)\n",
        "\n",
        "    # Inputs of the Siamese Network\n",
        "    input_a = Input(shape=(402, 128))\n",
        "    input_b = Input(shape=(402, 128))\n",
        "\n",
        "    # Process both inputs through the same base model\n",
        "    processed_a = base_model(input_a)\n",
        "    processed_b = base_model(input_b)\n",
        "\n",
        "    # Calculate the cosine similarity\n",
        "    cosine_sim = Lambda(cosine_similarity, output_shape=(1,))([processed_a, processed_b])\n",
        "\n",
        "    # Final Siamese network model\n",
        "    siamese_network = Model(inputs=[input_a, input_b], outputs=cosine_sim)\n",
        "\n",
        "    # Select and configure the optimizer\n",
        "    if params['optimizer']['type'] == 'adam':\n",
        "        optimizer = Adam(learning_rate=params['optimizer']['lr'])\n",
        "    elif params['optimizer']['type'] == 'sgd':\n",
        "        optimizer = SGD(learning_rate=params['optimizer']['lr'], momentum=params['optimizer']['momentum'])\n",
        "    elif params['optimizer']['type'] == 'rmsprop':\n",
        "        optimizer = RMSprop(learning_rate=params['optimizer']['lr'])\n",
        "\n",
        "    siamese_network.compile(optimizer=optimizer, loss=contrastive_loss)\n",
        "\n",
        "    # Dummy data placeholders, replace with actual data\n",
        "    x1_train, x2_train, y_train, x1_val, x2_val, y_val =train_pairs[:, 0], train_pairs[:, 1], train_pairs_labels,val_pairs[:, 0], val_pairs[:, 1], val_pairs_labels\n",
        "\n",
        "    # Training (assuming dummy placeholders are replaced)\n",
        "    result = siamese_network.fit([x1_train, x2_train], y_train, epochs=10, batch_size=params['batch_size'], validation_data=([x1_val, x2_val], y_val), verbose=0)\n",
        "    validation_loss = np.min(result.history['val_loss'])\n",
        "    return {'loss': validation_loss, 'status': STATUS_OK}\n",
        "\n",
        "# Define the search space for hyperparameters\n",
        "space = {\n",
        "    'num_layers': hp.choice('num_layers', range(3, 7)),  # Reduced range for simplicity\n",
        "    'num_filters': hp.choice('num_filters', [16, 32, 64, 128, 256]),\n",
        "    'kernel_size': hp.choice('kernel_size', [3, 5, 7]),\n",
        "    'batch_size': hp.choice('batch_size', [16, 32, 64]),\n",
        "    'optimizer': hp.choice('optimizer', [\n",
        "        {'type': 'adam', 'lr': hp.loguniform('adam_lr', np.log(1e-4), np.log(1e-2))},\n",
        "        {'type': 'sgd', 'lr': hp.loguniform('sgd_lr', np.log(1e-4), np.log(1e-2)), 'momentum': hp.uniform('sgd_momentum', 0.0, 0.99)},\n",
        "        {'type': 'rmsprop', 'lr': hp.loguniform('rmsprop_lr', np.log(1e-4), np.log(1e-2))}\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Running the hyperparameter optimization\n",
        "trials = Trials()\n",
        "best_params = fmin(\n",
        "    fn=objective,\n",
        "    space=space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=50,  # Adjust based on computational resources and needs\n",
        "    trials=trials\n",
        ")\n",
        "\n",
        "print(\"Best parameters:\", best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz678BBKsb5I",
        "outputId": "0db96c31-b89f-4ceb-cce5-8edf4e31399f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 54%|█████▍    | 27/50 [10:48<07:33, 19.72s/trial, best loss: 1.2576606499186739e-15]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv1D, BatchNormalization, MaxPooling1D, GlobalAveragePooling1D, Lambda\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
        "import keras.backend as K\n",
        "\n",
        "# Define the Siamese Network model architecture with variable layers, filters, and kernel size\n",
        "def siamese_model(params):\n",
        "    input_shape = (402, 128)  # Define the shape of your input vectors\n",
        "    model_input = Input(shape=input_shape)\n",
        "    x = model_input\n",
        "\n",
        "    # Dynamically adding layers based on num_layers\n",
        "    for i in range(params['num_layers']):\n",
        "        x = Conv1D(params['num_filters'], kernel_size=params['kernel_size'], activation=params['activation'], padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        if i < params['num_layers'] - 1:  # add max pooling to all but the last layer\n",
        "            x = MaxPooling1D(pool_size=2, strides=2)(x)\n",
        "\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    return Model(inputs=model_input, outputs=x)\n",
        "\n",
        "# Define cosine similarity function\n",
        "def cosine_similarity(vectors):\n",
        "    x, y = vectors\n",
        "    x = K.l2_normalize(x, axis=-1)\n",
        "    y = K.l2_normalize(y, axis=-1)\n",
        "    return K.sum(x * y, axis=-1, keepdims=True)\n",
        "\n",
        "# Define contrastive loss function\n",
        "def contrastive_loss(y_true, y_pred):\n",
        "    margin = 1.0\n",
        "    return K.mean(y_true * K.square(1 - y_pred) + (1 - y_true) * K.square(K.maximum(y_pred - margin, 0)))\n",
        "\n",
        "# Define the objective function for hyperparameter optimization\n",
        "def objective(params):\n",
        "    # Create the Siamese model with current params\n",
        "    base_model = siamese_model(params)\n",
        "\n",
        "    # Inputs of the Siamese Network\n",
        "    input_a = Input(shape=(402, 128))\n",
        "    input_b = Input(shape=(402, 128))\n",
        "\n",
        "    # Process both inputs through the same base model\n",
        "    processed_a = base_model(input_a)\n",
        "    processed_b = base_model(input_b)\n",
        "\n",
        "    # Calculate the cosine similarity\n",
        "    cosine_sim = Lambda(cosine_similarity, output_shape=(1,))([processed_a, processed_b])\n",
        "\n",
        "    # Final Siamese network model\n",
        "    siamese_network = Model(inputs=[input_a, input_b], outputs=cosine_sim)\n",
        "\n",
        "    # Select and configure the optimizer\n",
        "    if params['optimizer']['type'] == 'adam':\n",
        "        optimizer = Adam(learning_rate=params['optimizer']['lr'])\n",
        "    elif params['optimizer']['type'] == 'sgd':\n",
        "        optimizer = SGD(learning_rate=params['optimizer']['lr'], momentum=params['optimizer']['momentum'])\n",
        "    elif params['optimizer']['type'] == 'rmsprop':\n",
        "        optimizer = RMSprop(learning_rate=params['optimizer']['lr'])\n",
        "\n",
        "    siamese_network.compile(optimizer=optimizer, loss=contrastive_loss)\n",
        "\n",
        "    # Dummy data placeholders, replace with actual data\n",
        "    x1_train, x2_train, y_train, x1_val, x2_val, y_val =train_pairs[:, 0], train_pairs[:, 1], train_pairs_labels,val_pairs[:, 0], val_pairs[:, 1], val_pairs_labels\n",
        "\n",
        "    # Training (assuming dummy placeholders are replaced)\n",
        "    result = siamese_network.fit([x1_train, x2_train], y_train, epochs=10, batch_size=params['batch_size'], validation_data=([x1_val, x2_val], y_val), verbose=0)\n",
        "    validation_loss = np.min(result.history['val_loss'])\n",
        "    return {'loss': validation_loss, 'status': STATUS_OK}\n",
        "\n",
        "# Define the search space for hyperparameters\n",
        "space = {\n",
        "    'num_layers': hp.choice('num_layers', range(3, 7)),  # Reduced range for simplicity\n",
        "    'num_filters': hp.choice('num_filters', [16, 32, 64, 128, 256]),\n",
        "    'kernel_size': hp.choice('kernel_size', [3, 5, 7]),\n",
        "    'batch_size': hp.choice('batch_size', [16, 32, 64]),\n",
        "    'activation': hp.choice('activation', ['relu', 'tanh', 'sigmoid']),\n",
        "    'optimizer': hp.choice('optimizer', [\n",
        "        {'type': 'adam', 'lr': hp.loguniform('adam_lr', np.log(1e-4), np.log(1e-2))},\n",
        "        {'type': 'sgd', 'lr': hp.loguniform('sgd_lr', np.log(1e-4), np.log(1e-2)), 'momentum': hp.uniform('sgd_momentum', 0.0, 0.99)},\n",
        "        {'type': 'rmsprop', 'lr': hp.loguniform('rmsprop_lr', np.log(1e-4), np.log(1e-2))}\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Running the hyperparameter optimization\n",
        "trials = Trials()\n",
        "best_params = fmin(\n",
        "    fn=objective,\n",
        "    space=space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=10,  # Adjust based on computational resources and needs\n",
        "    trials=trials\n",
        ")\n",
        "\n",
        "print(\"Best parameters:\", best_params)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5fAaFn-7cc2",
        "outputId": "0edb0157-687b-47ea-98b8-55965db50d54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 10/10 [21:42<00:00, 130.26s/trial, best loss: 1.570299451959052e-15]\n",
            "Best parameters: {'activation': 2, 'adam_lr': 0.0006009855440680273, 'batch_size': 1, 'kernel_size': 0, 'num_filters': 2, 'num_layers': 2, 'optimizer': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After finding the best parameters, decode them to their actual meanings\n",
        "def decode_hyperparameters(params):\n",
        "    return {\n",
        "        'num_layers': range(3, 7)[params['num_layers']],\n",
        "        'num_filters': [16, 32, 64, 128, 256, 512][params['num_filters']],\n",
        "        'kernel_size': [3, 5, 7][params['kernel_size']],\n",
        "        'batch_size': [16, 32, 64][params['batch_size']],\n",
        "        #'optimizer': ['adam', 'sgd', 'rmsprop'][params['optimizer']['type']],\n",
        "       # 'lr': params['optimizer']['lr']  # This will need proper mapping if it's also indexed\n",
        "    }\n",
        "\n",
        "decoded_params = decode_hyperparameters(best_params)\n",
        "print(\"Decoded parameters:\", decoded_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iekcBnjTpW4",
        "outputId": "4f6d5e98-0dbe-4f1d-bdc6-ad24a9fd9690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoded parameters: {'num_layers': 5, 'num_filters': 64, 'kernel_size': 3, 'batch_size': 32}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sjczQm2NYqOF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}