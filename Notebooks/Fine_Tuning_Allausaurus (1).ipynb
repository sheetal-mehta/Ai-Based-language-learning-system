{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JQd4EPwzG-HH",
        "3GZc1hajHvQQ"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Requirements"
      ],
      "metadata": {
        "id": "JQd4EPwzG-HH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRNVXZvhFBQW",
        "outputId": "0f26d686-36ab-4706-c0c5-ad9eacd1155b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting allosaurus\n",
            "  Downloading allosaurus-1.0.2-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from allosaurus) (1.11.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from allosaurus) (1.25.2)\n",
            "Collecting resampy (from allosaurus)\n",
            "  Downloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting panphon (from allosaurus)\n",
            "  Downloading panphon-0.20.0-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from allosaurus) (2.3.0+cu121)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from allosaurus) (0.6.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from panphon->allosaurus) (67.7.2)\n",
            "Collecting unicodecsv (from panphon->allosaurus)\n",
            "  Downloading unicodecsv-0.14.1.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from panphon->allosaurus) (6.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from panphon->allosaurus) (2024.5.15)\n",
            "Collecting munkres (from panphon->allosaurus)\n",
            "  Downloading munkres-1.1.4-py2.py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: numba>=0.53 in /usr/local/lib/python3.10/dist-packages (from resampy->allosaurus) (0.58.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->allosaurus) (3.15.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->allosaurus) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->allosaurus) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->allosaurus) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->allosaurus) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->allosaurus) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->allosaurus)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->allosaurus)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->allosaurus)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->allosaurus)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->allosaurus)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->allosaurus)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->allosaurus)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->allosaurus)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->allosaurus)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->allosaurus)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->allosaurus)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->allosaurus) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->allosaurus)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.53->resampy->allosaurus) (0.41.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->allosaurus) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->allosaurus) (1.3.0)\n",
            "Building wheels for collected packages: unicodecsv\n",
            "  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-py3-none-any.whl size=10746 sha256=94b10d4b4fc300c51fdb002bbe41bbdee0ac7f171031604b4bc9e91bd0ea2b9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/ea/66/8e45247b09052a933eb1a680b7c64802298faba58aac9b346b\n",
            "Successfully built unicodecsv\n",
            "Installing collected packages: unicodecsv, munkres, panphon, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, resampy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, allosaurus\n",
            "Successfully installed allosaurus-1.0.2 munkres-1.1.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 panphon-0.20.0 resampy-0.4.3 unicodecsv-0.14.1\n"
          ]
        }
      ],
      "source": [
        "!pip install allosaurus"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installing latest model from releases"
      ],
      "metadata": {
        "id": "0G4NcmxxHCU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!python -m allosaurus.bin.list_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwORrS6WFvsp",
        "outputId": "79200977-fb01-4f67-a9d8-c849c356e43b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available Models\n",
            "- uni2005 (default)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m allosaurus.bin.download_model -m \"latest\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D5Dr8xiGDkO",
        "outputId": "477e05c9-6ff2-4cc9-a807-459261b6afa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading model  latest\n",
            "from:  https://github.com/xinjli/allosaurus/releases/download/v1.0/latest.tar.gz\n",
            "to:    /usr/local/lib/python3.10/dist-packages/allosaurus/pretrained\n",
            "please wait...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m allosaurus.bin.update_phone --lang \"deu\" --input \"/content/phone_dir\""
      ],
      "metadata": {
        "id": "pBp_5-ATTk0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m allosaurus.bin.list_phone --lang \"deu\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--S1858WLYFO",
        "outputId": "c6ce39a3-ebfe-40d3-9356-517afc58ea11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a aː b d d̠ d̺ e eː f h i iː j k kʰ l m n oː p pʰ s t tʰ t̠ u uː v x y yː z øː ŋ œ ɐ ɔ ə ɛ ɛː ɡ ɪ ʀ ʁ ʃ ʊ ʏ ʏː ʒ ʔ ʋ ɕ o ɔɪ ç ɑː ɑ r ts p͡f ø tʃ dʒ aɪ aʊ g t͡s ɔʏ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!python -m allosaurus.bin.write_phone --lang \"deu\" --output \"phone_dir\""
      ],
      "metadata": {
        "id": "U3s7iMbsQ7PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing train and test data for modelling"
      ],
      "metadata": {
        "id": "-Krz1fZPHHVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip data_100.zip"
      ],
      "metadata": {
        "id": "aHl_MfmG7pdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m allosaurus.bin.prep_feat --model=\"uni2005\" --path=\"/content/data/validate\""
      ],
      "metadata": {
        "id": "QgdPh9S0FLpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84f0e57d-1089-40a1-a277-2c3cb348fbbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 20/20 [00:05<00:00,  3.66it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "/content/test/SNM-test/test_20/clips1/common_voice_de_17639965.wav"
      ],
      "metadata": {
        "id": "LHlxywJc9epD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m allosaurus.bin.prep_feat --model=\"uni2005\" --path=\"/content/data/train\""
      ],
      "metadata": {
        "id": "Rft5ecy4HOeo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "551423cf-99e4-412f-95ee-a77d1a74422b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 100/100 [00:21<00:00,  4.57it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing train and test text features"
      ],
      "metadata": {
        "id": "MvkxhuxVHQTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m allosaurus.bin.prep_token --model=\"uni2005\" --lang=\"deu\" --path=\"/content/data/train\""
      ],
      "metadata": {
        "id": "UsfKR4qcHgpH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfd05055-d442-4974-cdd1-abc27888326a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r  0% 0/100 [00:00<?, ?it/s]\r100% 100/100 [00:00<00:00, 44705.86it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m allosaurus.bin.prep_token --model=\"uni2005\" --lang=\"deu\" --path=\"/content/data/validate\""
      ],
      "metadata": {
        "id": "M54rbd-uHiab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddcb7b10-b95c-4a9d-e2e9-a035de826311"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r  0% 0/20 [00:00<?, ?it/s]\r100% 20/20 [00:00<00:00, 31524.27it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "P3x8ZOgaHnTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m allosaurus.bin.adapt_model --pretrained_model=\"uni2005\" --new_model=\"allo_100_1\" --path=\"/content/data\" --lang=\"deu\" --device_id=0 --epoch=50"
      ],
      "metadata": {
        "id": "Hd6nCisxHmw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57c05adf-7e2f-4458-b29a-b1f77159091b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch[batch]: 00[0000] | train loss 4.10741 train per 0.59939\n",
            "epoch0 | validate per : 0.55771\n",
            "saving model\n",
            "epoch[batch]: 01[0000] | train loss 3.71454 train per 0.52862\n",
            "epoch1 | validate per : 0.57929\n",
            "epoch[batch]: 02[0000] | train loss 3.29648 train per 0.52213\n",
            "epoch2 | validate per : 0.54693\n",
            "saving model\n",
            "epoch[batch]: 03[0000] | train loss 3.17889 train per 0.51976\n",
            "epoch3 | validate per : 0.52751\n",
            "saving model\n",
            "epoch[batch]: 04[0000] | train loss 2.81023 train per 0.47119\n",
            "epoch4 | validate per : 0.50917\n",
            "saving model\n",
            "epoch[batch]: 05[0000] | train loss 2.72573 train per 0.49422\n",
            "epoch5 | validate per : 0.49515\n",
            "saving model\n",
            "epoch[batch]: 06[0000] | train loss 2.40107 train per 0.45058\n",
            "epoch6 | validate per : 0.49622\n",
            "epoch[batch]: 07[0000] | train loss 2.25515 train per 0.43481\n",
            "epoch7 | validate per : 0.48544\n",
            "saving model\n",
            "epoch[batch]: 08[0000] | train loss 2.28269 train per 0.43502\n",
            "epoch8 | validate per : 0.47896\n",
            "saving model\n",
            "epoch[batch]: 09[0000] | train loss 2.02656 train per 0.40995\n",
            "epoch9 | validate per : 0.47249\n",
            "saving model\n",
            "epoch[batch]: 10[0000] | train loss 2.06033 train per 0.43647\n",
            "epoch10 | validate per : 0.46278\n",
            "saving model\n",
            "epoch[batch]: 11[0000] | train loss 1.84477 train per 0.38387\n",
            "epoch11 | validate per : 0.45847\n",
            "saving model\n",
            "epoch[batch]: 12[0000] | train loss 1.77576 train per 0.37356\n",
            "epoch12 | validate per : 0.45200\n",
            "saving model\n",
            "epoch[batch]: 13[0000] | train loss 1.82149 train per 0.37508\n",
            "epoch13 | validate per : 0.43905\n",
            "saving model\n",
            "epoch[batch]: 14[0000] | train loss 1.74498 train per 0.38359\n",
            "epoch14 | validate per : 0.43042\n",
            "saving model\n",
            "epoch[batch]: 15[0000] | train loss 1.67763 train per 0.37629\n",
            "epoch15 | validate per : 0.43366\n",
            "epoch[batch]: 16[0000] | train loss 1.48618 train per 0.32141\n",
            "epoch16 | validate per : 0.43150\n",
            "epoch[batch]: 17[0000] | train loss 1.52925 train per 0.33468\n",
            "epoch17 | validate per : 0.42718\n",
            "saving model\n",
            "epoch[batch]: 18[0000] | train loss 1.35666 train per 0.30625\n",
            "epoch18 | validate per : 0.42179\n",
            "saving model\n",
            "epoch[batch]: 19[0000] | train loss 1.42149 train per 0.34286\n",
            "epoch19 | validate per : 0.42071\n",
            "saving model\n",
            "epoch[batch]: 20[0000] | train loss 1.33300 train per 0.30976\n",
            "epoch20 | validate per : 0.41855\n",
            "saving model\n",
            "epoch[batch]: 21[0000] | train loss 1.18442 train per 0.27714\n",
            "epoch21 | validate per : 0.41424\n",
            "saving model\n",
            "epoch[batch]: 22[0000] | train loss 1.26714 train per 0.31611\n",
            "epoch22 | validate per : 0.41532\n",
            "epoch[batch]: 23[0000] | train loss 1.16767 train per 0.28687\n",
            "epoch23 | validate per : 0.41316\n",
            "saving model\n",
            "epoch[batch]: 24[0000] | train loss 1.11935 train per 0.27946\n",
            "epoch24 | validate per : 0.40885\n",
            "saving model\n",
            "epoch[batch]: 25[0000] | train loss 1.13205 train per 0.29119\n",
            "epoch25 | validate per : 0.41424\n",
            "epoch[batch]: 26[0000] | train loss 1.09296 train per 0.28328\n",
            "epoch26 | validate per : 0.41316\n",
            "epoch[batch]: 27[0000] | train loss 0.92057 train per 0.23772\n",
            "epoch27 | validate per : 0.40453\n",
            "saving model\n",
            "epoch[batch]: 28[0000] | train loss 0.94290 train per 0.24579\n",
            "epoch28 | validate per : 0.40022\n",
            "saving model\n",
            "epoch[batch]: 29[0000] | train loss 0.97812 train per 0.25410\n",
            "epoch29 | validate per : 0.39806\n",
            "saving model\n",
            "epoch[batch]: 30[0000] | train loss 0.93931 train per 0.24559\n",
            "epoch30 | validate per : 0.39806\n",
            "saving model\n",
            "epoch[batch]: 31[0000] | train loss 0.82345 train per 0.21684\n",
            "epoch31 | validate per : 0.40345\n",
            "epoch[batch]: 32[0000] | train loss 0.75426 train per 0.20194\n",
            "epoch32 | validate per : 0.40345\n",
            "epoch[batch]: 33[0000] | train loss 0.75653 train per 0.20202\n",
            "epoch33 | validate per : 0.39914\n",
            "no improvements for several epochs, early stopping now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the model"
      ],
      "metadata": {
        "id": "3GZc1hajHvQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m allosaurus.bin.list_model"
      ],
      "metadata": {
        "id": "28cc7stuHuuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import allosaurus\n",
        "from allosaurus.app import read_recognizer\n",
        "model = read_recognizer(\"your_new_model_name\")"
      ],
      "metadata": {
        "id": "mSPyAKxpH2m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.recognize(path_to_voice_file,'deu')"
      ],
      "metadata": {
        "id": "hTwMhsZoH49l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "\n",
        "def levenshtein_distance(reference: List[str], hypothesis: List[str]) -> Tuple[int, int, int]:\n",
        "    \"\"\"\n",
        "    Calculate the Levenshtein distance between two lists of phonemes.\n",
        "    \"\"\"\n",
        "    ref_len = len(reference)\n",
        "    hyp_len = len(hypothesis)\n",
        "\n",
        "    # Create a distance matrix\n",
        "    dist_matrix = np.zeros((ref_len + 1, hyp_len + 1), dtype=int)\n",
        "\n",
        "    # Initialize the distance matrix\n",
        "    for i in range(ref_len + 1):\n",
        "        dist_matrix[i][0] = i\n",
        "    for j in range(hyp_len + 1):\n",
        "        dist_matrix[0][j] = j\n",
        "\n",
        "    # Populate the distance matrix\n",
        "    for i in range(1, ref_len + 1):\n",
        "        for j in range(1, hyp_len + 1):\n",
        "            if reference[i - 1] == hypothesis[j - 1]:\n",
        "                cost = 0\n",
        "            else:\n",
        "                cost = 1\n",
        "            dist_matrix[i][j] = min(dist_matrix[i - 1][j] + 1,      # Deletion\n",
        "                                    dist_matrix[i][j - 1] + 1,      # Insertion\n",
        "                                    dist_matrix[i - 1][j - 1] + cost)  # Substitution\n",
        "\n",
        "    # The distance is the value in the bottom right corner of the matrix\n",
        "    distance = dist_matrix[ref_len][hyp_len]\n",
        "\n",
        "    # Backtrack to find the number of insertions, deletions, and substitutions\n",
        "    i, j = ref_len, hyp_len\n",
        "    insertions = deletions = substitutions = 0\n",
        "\n",
        "    while i > 0 or j > 0:\n",
        "        if i > 0 and j > 0 and dist_matrix[i][j] == dist_matrix[i - 1][j - 1] + (1 if reference[i - 1] != hypothesis[j - 1] else 0):\n",
        "            if reference[i - 1] != hypothesis[j - 1]:\n",
        "                substitutions += 1\n",
        "            i -= 1\n",
        "            j -= 1\n",
        "        elif i > 0 and dist_matrix[i][j] == dist_matrix[i - 1][j] + 1:\n",
        "            deletions += 1\n",
        "            i -= 1\n",
        "        else:\n",
        "            insertions += 1\n",
        "            j -= 1\n",
        "\n",
        "    return insertions, deletions, substitutions\n",
        "\n",
        "def phoneme_error_rate(reference: List[str], hypothesis: List[str]) -> float:\n",
        "    \"\"\"\n",
        "    Calculate the Phoneme Error Rate (PER).\n",
        "\n",
        "    :param reference: List of phonemes in the reference transcription.\n",
        "    :param hypothesis: List of phonemes in the hypothesis transcription.\n",
        "    :return: Phoneme Error Rate as a float.\n",
        "    \"\"\"\n",
        "    insertions, deletions, substitutions = levenshtein_distance(reference, hypothesis)\n",
        "    total_errors = insertions + deletions + substitutions\n",
        "    total_phonemes = len(reference)\n",
        "\n",
        "    return total_errors / total_phonemes\n",
        "\n",
        "# Example usage\n",
        "reference_phonemes = [\"p\", \"h\", \"o\", \"n\", \"e\", \"m\", \"e\"]\n",
        "hypothesis_phonemes = [\"p\", \"h\", \"o\", \"m\", \"e\", \"n\", \"e\"]\n",
        "\n",
        "per = phoneme_error_rate(reference_phonemes, hypothesis_phonemes)\n",
        "print(f\"Phoneme Error Rate: {per:.2%}\")\n"
      ],
      "metadata": {
        "id": "NEguyvtjIRB4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}